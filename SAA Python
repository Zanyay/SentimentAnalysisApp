import re
from collections import defaultdict
from typing import List, Dict, Tuple
import tkinter as tk
from tkinter import messagebox, filedialog
from tkinter import ttk
import speech_recognition as sr
from pathlib import Path
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import numpy as np
from matplotlib.figure import Figure
import math
import csv
import queue  # Add new import
import threading  # Add new import

class SentimentAnalyzer:
    def __init__(self):
        self.sentiment_data = {
            'positive': [],
            'neutral': [],
            'negative': []
        }
        self.ngram_counts = {
            'bigram': defaultdict(lambda: defaultdict(int)),
            'trigram': defaultdict(lambda: defaultdict(int))
        }

        # Initialize Naive Bayes and SVM models
        self.vectorizer = TfidfVectorizer()
        self.nb_model = MultinomialNB()
        self.svm_model = make_pipeline(TfidfVectorizer(), SVC(probability=True))

        self.load_training_data()
        self.train()

    def load_training_data(self):
        # Load training data in CSV format
        training_data = [
            ["Sentence", "Sentiment"],  # Header row
            ["I absolutely love this product!", "positive"],
            ["This is the best day ever!", "positive"],
            ["Great service and amazing quality", "positive"],
            ["The weather is okay today", "neutral"],
            ["I'm not sure how I feel about this", "neutral"],
            ["It could be better but it's not terrible", "neutral"],
            ["This is terrible, I hate it", "negative"],
            ["Worst experience ever, very disappointing", "negative"],
            ["Poor quality and bad service", "negative"],
            ["I love my phone", "positive"],
            ["I hate my phone", "negative"],
            ["My phone is okay", "neutral"],
            ["I am very happy with my purchase", "positive"],
            ["I am very unhappy with my purchase", "negative"],
            ["The product is average", "neutral"]
        ]

        # Skip header row and process data
        for text, sentiment in training_data[1:]:
            self.sentiment_data[sentiment.lower()].append(text.lower())

    def preprocess_text(self, text: str) -> str:
        # Clean and preprocess text
        text = text.lower()
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        return text

    def get_ngrams(self, text: str, n: int) -> List[str]:
        # Generate n-grams from text
        words = self.preprocess_text(text).split()
        ngrams = []
        for i in range(len(words) - n + 1):
            ngram = ' '.join(words[i:i + n])
            ngrams.append(ngram)
        return ngrams

    def train(self):
        # Train the N-gram model
        for sentiment, texts in self.sentiment_data.items():
            for text in texts:
                # Process bigrams
                bigrams = self.get_ngrams(text, 2)
                for bigram in bigrams:
                    self.ngram_counts['bigram'][sentiment][bigram] += 1

                # Process trigrams
                trigrams = self.get_ngrams(text, 3)
                for trigram in trigrams:
                    self.ngram_counts['trigram'][sentiment][trigram] += 1

        # Train Naive Bayes and SVM models
        all_texts = []
        all_labels = []
        for sentiment, texts in self.sentiment_data.items():
            all_texts.extend(texts)
            all_labels.extend([sentiment] * len(texts))
        self.nb_model.fit(self.vectorizer.fit_transform(all_texts), all_labels)
        self.svm_model.fit(all_texts, all_labels)

    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        # Analyze sentiment using N-gram method
        bigrams = self.get_ngrams(text, 2)
        trigrams = self.get_ngrams(text, 3)
        
        scores_ngram = {
            'positive': 0.0,
            'neutral': 0.0,
            'negative': 0.0
        }

        bigram_weight = 0.75
        trigram_weight = 0.25

        for bigram in bigrams:
            for sentiment in scores_ngram.keys():
                if bigram in self.ngram_counts['bigram'][sentiment]:
                    scores_ngram[sentiment] += self.ngram_counts['bigram'][sentiment][bigram] * bigram_weight

        for trigram in trigrams:
            for sentiment in scores_ngram.keys():
                if trigram in self.ngram_counts['trigram'][sentiment]:
                    scores_ngram[sentiment] += self.ngram_counts['trigram'][sentiment][trigram] * trigram_weight

        if sum(scores_ngram.values()) == 0:
            scores_ngram['neutral'] = 1.0

        total_score_ngram = sum(scores_ngram.values()) or 1
        normalized_scores_ngram = {
            sentiment: (score / total_score_ngram) * 100 
            for sentiment, score in scores_ngram.items()
        }

        # Analyze sentiment using Naive Bayes method
        nb_probs = self.nb_model.predict_proba(self.vectorizer.transform([text]))[0]
        scores_nb = {
            'positive': nb_probs[self.nb_model.classes_.tolist().index('positive')] * 100,
            'neutral': nb_probs[self.nb_model.classes_.tolist().index('neutral')] * 100,
            'negative': nb_probs[self.nb_model.classes_.tolist().index('negative')] * 100
        }

        # Analyze sentiment using SVM method
        svm_probs = self.svm_model.predict_proba([text])[0]
        scores_svm = {
            'positive': svm_probs[self.svm_model.classes_.tolist().index('positive')] * 100,
            'neutral': svm_probs[self.svm_model.classes_.tolist().index('neutral')] * 100,
            'negative': svm_probs[self.svm_model.classes_.tolist().index('negative')] * 100
        }

        # Average the results for final consensus
        final_scores = {
            sentiment: (normalized_scores_ngram[sentiment] + scores_nb[sentiment] + scores_svm[sentiment]) / 3
            for sentiment in scores_ngram.keys()
        }

        return {
            'ngram': normalized_scores_ngram,
            'naive_bayes': scores_nb,
            'svm': scores_svm,
            'final': final_scores
        }

    def get_final_sentiment(self, scores: Dict[str, float]) -> Tuple[str, float]:
        final_sentiment = max(scores.items(), key=lambda x: x[1])
        return final_sentiment

    def add_to_training_data(self, text: str, sentiment: str):
        # Add new text to training data and retrain the model
        self.sentiment_data[sentiment].append(text.lower())
        self.train()

    def export_training_data(self, filepath: str):
        # Export training data to a CSV file
        with open(filepath, 'w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(["Sentence", "Sentiment"])  # Write header
            for sentiment, texts in self.sentiment_data.items():
                for text in texts:
                    writer.writerow([text, sentiment])

    def import_training_data(self, filepath: str):
        if not Path(filepath).exists():
            messagebox.showerror("Error", f"File not found: {filepath}")
            return
        
        # Create progress window
        progress_window = tk.Toplevel()
        progress_window.title("Importing Data")
        progress_window.geometry("300x150")
        progress_window.transient(root)  # Make window modal
        progress_window.grab_set()  # Make window modal
        
        progress_label = ttk.Label(progress_window, text="Reading CSV file...")
        progress_label.pack(pady=10)
        
        # Create frame for progress bar and percentage
        progress_frame = ttk.Frame(progress_window)
        progress_frame.pack(pady=10)
        
        progress_bar = ttk.Progressbar(progress_frame, length=200, mode='determinate')
        progress_bar.pack(side=tk.LEFT, padx=(0, 5))
        
        percentage_label = ttk.Label(progress_frame, text="0%")
        percentage_label.pack(side=tk.LEFT)
        
        try:
            # First count total lines for progress tracking
            with open(filepath, 'r', encoding='utf-8-sig') as file:  # Use utf-8-sig to handle BOM
                total_lines = sum(1 for _ in file) - 1  # Subtract header
            progress_bar['maximum'] = total_lines
            
            # Process in batches
            BATCH_SIZE = 1000
            current_batch = []
            processed_lines = 0
            
            with open(filepath, 'r', encoding='utf-8-sig') as file:  # Use utf-8-sig to handle BOM
                reader = csv.DictReader(file)
                
                for row in reader:
                    sentiment = row['Sentiment'].lower()
                    text = row['Sentence'].lower()
                    
                    if sentiment in self.sentiment_data:
                        current_batch.append((text, sentiment))
                        processed_lines += 1
                        
                        if len(current_batch) >= BATCH_SIZE:
                            # Process batch
                            for text, sent in current_batch:
                                self.sentiment_data[sent].append(text)
                            current_batch = []
                            
                            # Update progress
                            progress_bar['value'] = processed_lines
                            percentage = min(100, int((processed_lines / total_lines) * 100))
                            percentage_label.config(text=f"{percentage}%")
                            progress_label.config(text=f"Processed {processed_lines:,} of {total_lines:,} entries")
                            progress_window.update()
                
                # Process remaining items
                for text, sent in current_batch:
                    self.sentiment_data[sent].append(text)
                
                # Ensure progress bar shows 100%
                progress_bar['value'] = total_lines
                percentage_label.config(text="100%")
                progress_label.config(text=f"Processed {total_lines:,} of {total_lines:,} entries")
                progress_window.update()
            
            progress_label.config(text="Training model...")
            progress_window.update()
            
            def cleanup():
                try:
                    root.after(1, progress_window.destroy)  # Schedule window destruction
                    root.after(200, update_training_data_view)  # Update the view after cleanup
                except:
                    pass
            
            # Train in a separate thread to prevent UI freeze
            def train_thread():
                try:
                    self.train()
                    root.after(1, cleanup)  # Schedule cleanup on main thread using root instead of progress_window
                except Exception as e:
                    root.after(1, progress_window.destroy)
                    root.after(100, lambda: messagebox.showerror("Error", f"Error training model: {str(e)}"))
            
            training_thread = threading.Thread(target=train_thread, daemon=True)
            training_thread.start()
            
        except Exception as e:
            progress_window.destroy()
            messagebox.showerror("Error", f"Error reading CSV file: {str(e)}")
            return

    def get_training_data(self) -> List[Tuple[str, str]]:
        # Get the training data as a list of tuples
        training_data = []
        for sentiment, texts in self.sentiment_data.items():
            for text in texts:
                training_data.append((text, sentiment))
        return training_data

def create_about_tab(tab):
    # Create a scrollable frame
    canvas = tk.Canvas(tab)
    scrollbar = ttk.Scrollbar(tab, orient="vertical", command=canvas.yview)
    scrollable_frame = ttk.Frame(canvas)

    scrollable_frame.bind(
        "<Configure>",
        lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
    )

    canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
    canvas.configure(yscrollcommand=scrollbar.set)

    # Title
    title = ttk.Label(
        scrollable_frame,
        text="About the Sentiment Analyzer",
        font=("Helvetica", 14, "bold")
    )
    title.pack(pady=10)

    # What is Sentiment Analysis section
    what_is = ttk.LabelFrame(scrollable_frame, text="What is Sentiment Analysis?")
    ttk.Label(
        what_is,
        text="Sentiment analysis, also known as opinion mining, is a field within natural language processing (NLP) "
             "that focuses on identifying and extracting subjective information from textual data. Its primary goal "
             "is to determine the emotional tone or attitude expressed in a piece of text, categorizing it as positive, "
             "negative, or neutral. This tool combines three different approaches to sentiment analysis:",
        wraplength=500
    ).pack(pady=5)
    
    approaches = ttk.Frame(what_is)
    ttk.Label(approaches, text="• Naive Bayes Classification (probabilistic approach)").pack(anchor="w")
    ttk.Label(approaches, text="• N-gram Analysis (sequence-based approach)").pack(anchor="w")
    ttk.Label(approaches, text="• Support Vector Machine (geometric approach)").pack(anchor="w")
    approaches.pack(pady=5)
    what_is.pack(fill="x", padx=10, pady=5)

    # Naive Bayes Analysis section
    naive_bayes = ttk.LabelFrame(scrollable_frame, text="Naive Bayes Analysis")
    ttk.Label(
        naive_bayes,
        text="Naive Bayes is a probabilistic classifier based on Bayes' Theorem. It calculates the probability "
             "of a text belonging to each sentiment class based on the frequency of words in the training data. "
             "The model is called 'naive' because it assumes word independence, meaning the presence of one word "
             "is considered independent of other words.",
        wraplength=500
    ).pack(pady=5)
    
    formula = ttk.Label(
        naive_bayes,
        text="P(sentiment|text) = P(sentiment) × ∏ P(word|sentiment)",
        font=("Courier", 10, "bold")
    )
    formula.pack(pady=5)
    
    nb_components = ttk.Frame(naive_bayes)
    ttk.Label(nb_components, text="Key Features:").pack(anchor="w")
    ttk.Label(nb_components, text="• Simple and fast training/prediction").pack(anchor="w")
    ttk.Label(nb_components, text="• Effective with small datasets").pack(anchor="w")
    ttk.Label(nb_components, text="• Handles high-dimensional data well").pack(anchor="w")
    ttk.Label(nb_components, text="• Uses TF-IDF for word importance weighting").pack(anchor="w")
    ttk.Label(nb_components, text="• Applies Laplace smoothing for unseen words").pack(anchor="w")
    nb_components.pack(pady=5)
    naive_bayes.pack(fill="x", padx=10, pady=5)

    # N-gram Analysis section
    ngram = ttk.LabelFrame(scrollable_frame, text="N-gram Analysis")
    ttk.Label(
        ngram,
        text="N-gram analysis examines sequences of consecutive words to understand context and sentiment patterns. "
             "Unlike single-word approaches, n-grams can capture some level of word order and local context, which "
             "is particularly important for sentiment expression.",
        wraplength=500
    ).pack(pady=5)
    
    formula = ttk.Label(
        ngram,
        text="Score = 0.75 × Bigram_Score + 0.25 × Trigram_Score",
        font=("Courier", 10, "bold")
    )
    formula.pack(pady=5)
    
    ngram_types = ttk.Frame(ngram)
    ttk.Label(ngram_types, text="N-gram Types:").pack(anchor="w")
    ttk.Label(ngram_types, text="• Bigrams: Two-word sequences (e.g., 'very good')").pack(anchor="w")
    ttk.Label(ngram_types, text="• Trigrams: Three-word sequences (e.g., 'not very good')").pack(anchor="w")
    ttk.Label(ngram_types, text="• Weighted combination favors bigrams (75%) over trigrams (25%)").pack(anchor="w")
    ttk.Label(ngram_types, text="• Captures local context and common sentiment expressions").pack(anchor="w")
    ngram_types.pack(pady=5)
    ngram.pack(fill="x", padx=10, pady=5)

    # SVM Analysis section
    svm = ttk.LabelFrame(scrollable_frame, text="Support Vector Machine Analysis")
    ttk.Label(
        svm,
        text="Support Vector Machine (SVM) is a powerful classifier that finds an optimal hyperplane in a "
             "high-dimensional space to separate different sentiment classes. Through kernel functions, SVM can "
             "handle both linear and non-linear sentiment patterns.",
        wraplength=500
    ).pack(pady=5)
    
    formula = ttk.Label(
        svm,
        text="f(x) = Σ(αᵢ × K(xᵢ, x) + b)",
        font=("Courier", 10, "bold")
    )
    formula.pack(pady=5)
    
    svm_components = ttk.Frame(svm)
    ttk.Label(svm_components, text="Key Features:").pack(anchor="w")
    ttk.Label(svm_components, text="• Effective in high-dimensional spaces").pack(anchor="w")
    ttk.Label(svm_components, text="• Handles non-linear sentiment patterns").pack(anchor="w")
    ttk.Label(svm_components, text="• Uses kernel functions for complex pattern recognition").pack(anchor="w")
    ttk.Label(svm_components, text="• Maximizes the margin between sentiment classes").pack(anchor="w")
    ttk.Label(svm_components, text="• Robust to outliers in the training data").pack(anchor="w")
    svm_components.pack(pady=5)
    svm.pack(fill="x", padx=10, pady=5)

    # Final Consensus section
    consensus = ttk.LabelFrame(scrollable_frame, text="Final Consensus")
    ttk.Label(
        consensus,
        text="The final sentiment combines all three methods to leverage their individual strengths:\n"
             "• N-grams capture local context and common phrases\n"
             "• Naive Bayes provides robust probabilistic classification\n"
             "• SVM handles complex, non-linear sentiment patterns",
        wraplength=500
    ).pack(pady=5)
    
    formula = ttk.Label(
        consensus,
        text="Final_Score = (N-gram_Score + NB_Score + SVM_Score) / 3",
        font=("Courier", 10, "bold")
    )
    formula.pack(pady=5)
    consensus.pack(fill="x", padx=10, pady=5)

    # Pack the canvas and scrollbar
    canvas.pack(side="left", fill="both", expand=True, padx=5, pady=5)
    scrollbar.pack(side="right", fill="y")

def create_numbers_tab(tab):
    global frames  # Make frames global so it can be accessed from other functions
    frames = {}  # Store frames in a dictionary to maintain references
    
    # Create a notebook for sub-tabs within Numbers tab
    numbers_notebook = ttk.Notebook(tab)
    
    # Create sub-tabs for each model
    ngram_tab = ttk.Frame(numbers_notebook)
    nb_tab = ttk.Frame(numbers_notebook)
    svm_tab = ttk.Frame(numbers_notebook)
    
    numbers_notebook.add(ngram_tab, text="N-gram Details")
    numbers_notebook.add(nb_tab, text="Naive Bayes Details")
    numbers_notebook.add(svm_tab, text="SVM Details")
    numbers_notebook.pack(expand=1, fill="both")

    # Create and name the frames
    frames['ngram'] = ttk.Frame(ngram_tab, name='ngram')
    frames['naive_bayes'] = ttk.Frame(nb_tab, name='naive_bayes')
    frames['svm'] = ttk.Frame(svm_tab, name='svm')

    for frame in frames.values():
        frame.pack(fill=tk.BOTH, expand=1)

    def update_viz():
        if not last_analyzed_text or not last_analyzed_scores:
            return

        try:
            # Update all tabs at once
            update_viz_content(frames['ngram'], 'ngram')
            update_viz_content(frames['naive_bayes'], 'naive_bayes')
            update_viz_content(frames['svm'], 'svm')
        except Exception as e:
            print(f"Error updating visualization: {str(e)}")

    # Store the update_viz function as an attribute of the tab
    tab.update_viz = update_viz

    # Initialize all tabs
    update_viz()

    def update_viz_content(frame, method):
        # Clear existing widgets
        for widget in frame.winfo_children():
            widget.destroy()

        # Create a main frame for content
        main_content = ttk.Frame(frame)
        main_content.pack(fill=tk.BOTH, expand=True)

        # Create left and right columns
        left_column = ttk.Frame(main_content)
        right_column = ttk.Frame(main_content)
        left_column.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)
        right_column.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)

        # Formula and Explanation (Left Column)
        formula_frame = ttk.LabelFrame(left_column, text="Model Formula")
        formula_frame.pack(fill=tk.X, pady=5)

        if method == 'ngram':
            formula_text = "Score = 0.75 × Bigram_Score + 0.25 × Trigram_Score"
            explanation = "Weighted combination of bigram and trigram frequencies"
            
            # Get all n-grams from the text
            bigrams = analyzer.get_ngrams(last_analyzed_text, 2)
            trigrams = analyzer.get_ngrams(last_analyzed_text, 3)
            
            # Calculate bigram confidences
            bigram_details = "\nBigram Analysis:"
            for bigram in bigrams:
                # Get total occurrences of this bigram across all sentiments
                total_occurrences = sum(
                    analyzer.ngram_counts['bigram'][sentiment][bigram]
                    for sentiment in ['positive', 'neutral', 'negative']
                )
                
                if total_occurrences > 0:  # Only show if bigram exists in training data
                    bigram_details += f"\n\n'{bigram}' appears {total_occurrences} time{'s' if total_occurrences != 1 else ''} in training data:"
                    for sentiment in ['positive', 'neutral', 'negative']:
                        count = analyzer.ngram_counts['bigram'][sentiment][bigram]
                        percentage = (count / total_occurrences) * 100
                        bigram_details += f"\n  {sentiment}: {count} time{'s' if count != 1 else ''} ({percentage:.1f}%)"
                else:
                    bigram_details += f"\n\n'{bigram}' not found in training data"

            # Calculate trigram confidences
            trigram_details = "\n\nTrigram Analysis:"
            for trigram in trigrams:
                # Get total occurrences of this trigram across all sentiments
                total_occurrences = sum(
                    analyzer.ngram_counts['trigram'][sentiment][trigram]
                    for sentiment in ['positive', 'neutral', 'negative']
                )
                
                if total_occurrences > 0:  # Only show if trigram exists in training data
                    trigram_details += f"\n\n'{trigram}' appears {total_occurrences} time{'s' if total_occurrences != 1 else ''} in training data:"
                    for sentiment in ['positive', 'neutral', 'negative']:
                        count = analyzer.ngram_counts['trigram'][sentiment][trigram]
                        if count > 0:  # Only show non-zero counts
                            percentage = (count / total_occurrences) * 100
                            trigram_details += f"\n  {sentiment}: {count} time{'s' if count != 1 else ''} ({percentage:.1f}%)"
                else:
                    trigram_details += f"\n\n'{trigram}' not found in training data"

            calc_details = (
                f"Text analyzed: '{last_analyzed_text}'\n"
                f"{bigram_details}\n"
                f"{trigram_details}\n\n"
                f"Final weighted scores:\n"
                f"Positive: {last_analyzed_scores['ngram']['positive']:.2f}%\n"
                f"Neutral: {last_analyzed_scores['ngram']['neutral']:.2f}%\n"
                f"Negative: {last_analyzed_scores['ngram']['negative']:.2f}%"
            )

            # Graph (Bottom) - Modified to show 3 separate graphs
            graph_frame = ttk.LabelFrame(main_content, text="Visualization")
            graph_frame.pack(fill=tk.BOTH, expand=True, pady=5)

            # Create figure with 3 subplots
            fig = Figure(figsize=(6, 8))
            
            # Bigram graph
            ax1 = fig.add_subplot(311)  # 3 rows, 1 column, first plot
            sentiments = ['positive', 'neutral', 'negative']
            bigram_values = []
            for sentiment in sentiments:
                # Calculate total bigram score for each sentiment
                total = sum(analyzer.ngram_counts['bigram'][sentiment][bigram] for bigram in bigrams)
                bigram_values.append(total)
            
            total_bigram = sum(bigram_values) or 1
            bigram_percentages = [v * 100 / total_bigram for v in bigram_values]
            
            bars = ax1.bar(np.arange(len(sentiments)), bigram_percentages)
            ax1.set_xticks(np.arange(len(sentiments)))
            ax1.set_xticklabels(sentiments)
            ax1.set_title('Bigram Analysis')
            ax1.set_ylabel('Percentage (%)')
            
            for bar in bars:
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.1f}%', ha='center', va='bottom')

            # Trigram graph
            ax2 = fig.add_subplot(312)  # 3 rows, 1 column, second plot
            trigram_values = []
            for sentiment in sentiments:
                # Calculate total trigram score for each sentiment by summing all trigrams
                total = sum(
                    analyzer.ngram_counts['trigram'][sentiment][t] 
                    for t in trigrams
                )
                trigram_values.append(total)
            
            total_trigram = sum(trigram_values) or 1
            trigram_percentages = [v * 100 / total_trigram for v in trigram_values]
            
            bars = ax2.bar(np.arange(len(sentiments)), trigram_percentages)
            ax2.set_xticks(np.arange(len(sentiments)))
            ax2.set_xticklabels(sentiments)
            ax2.set_title('Trigram Analysis')
            ax2.set_ylabel('Percentage (%)')
            
            for bar in bars:
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.1f}%', ha='center', va='bottom')

            # Final weighted scores graph
            ax3 = fig.add_subplot(313)  # 3 rows, 1 column, third plot
            values = [last_analyzed_scores[method][s] for s in sentiments]
            
            bars = ax3.bar(np.arange(len(sentiments)), values)
            ax3.set_xticks(np.arange(len(sentiments)))
            ax3.set_xticklabels(sentiments)
            ax3.set_title('Final Weighted Scores (75% Bigram + 25% Trigram)')
            ax3.set_ylabel('Score (%)')
            
            for bar in bars:
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.1f}%', ha='center', va='bottom')

            # Adjust layout to prevent overlap
            fig.tight_layout(pad=3.0)

            canvas = FigureCanvasTkAgg(fig, master=graph_frame)
            canvas.draw()
            canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        elif method == 'naive_bayes':
            formula_text = "P(sentiment|text) = P(sentiment) × ∏ P(word|sentiment)"
            explanation = "Probability calculation using Bayes' theorem with word frequencies"
            words = [w for w in last_analyzed_text.lower().split() if w in analyzer.vectorizer.get_feature_names_out()]
            calc_details = (
                f"Text analyzed: '{last_analyzed_text}'\n\n"
                f"Words analyzed: {', '.join(words)}\n\n"
                f"Class probabilities:\n"
                f"Positive: {last_analyzed_scores['naive_bayes']['positive']:.2f}%\n"
                f"Neutral: {last_analyzed_scores['naive_bayes']['neutral']:.2f}%\n"
                f"Negative: {last_analyzed_scores['naive_bayes']['negative']:.2f}%"
            )

            # Keep original single graph for naive_bayes
            graph_frame = ttk.LabelFrame(main_content, text="Visualization")
            graph_frame.pack(fill=tk.BOTH, expand=True, pady=5)

            fig = Figure(figsize=(8, 4))
            ax = fig.add_subplot(111)

            sentiments = ['positive', 'neutral', 'negative']
            values = [last_analyzed_scores[method][s] for s in sentiments]
            x = np.arange(len(sentiments))
            
            bars = ax.bar(x, values)
            ax.set_xticks(x)
            ax.set_xticklabels(sentiments)
            ax.set_title(f'{method.replace("_", " ").title()} Analysis Scores')
            ax.set_ylabel('Probability (%)')

            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.1f}%', ha='center', va='bottom')

            canvas = FigureCanvasTkAgg(fig, master=graph_frame)
            canvas.draw()
            canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        else:  # SVM
            formula_text = "f(x) = Σ(αᵢ × K(xᵢ, x) + b)"
            explanation = "Support Vector Machine with TF-IDF features and RBF kernel"
            calc_details = (
                f"Text analyzed: '{last_analyzed_text}'\n\n"
                f"TF-IDF Features: {len(analyzer.svm_model.named_steps['tfidfvectorizer'].get_feature_names_out())} total features\n\n"
                f"Classification probabilities:\n"
                f"Positive: {last_analyzed_scores['svm']['positive']:.2f}%\n"
                f"Neutral: {last_analyzed_scores['svm']['neutral']:.2f}%\n"
                f"Negative: {last_analyzed_scores['svm']['negative']:.2f}%"
            )

            # Keep original single graph for SVM
            graph_frame = ttk.LabelFrame(main_content, text="Visualization")
            graph_frame.pack(fill=tk.BOTH, expand=True, pady=5)

            fig = Figure(figsize=(8, 4))
            ax = fig.add_subplot(111)

            sentiments = ['positive', 'neutral', 'negative']
            values = [last_analyzed_scores[method][s] for s in sentiments]
            x = np.arange(len(sentiments))
            
            bars = ax.bar(x, values)
            ax.set_xticks(x)
            ax.set_xticklabels(sentiments)
            ax.set_title(f'{method.replace("_", " ").title()} Analysis Scores')
            ax.set_ylabel('Probability (%)')

            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.1f}%', ha='center', va='bottom')

            canvas = FigureCanvasTkAgg(fig, master=graph_frame)
            canvas.draw()
            canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        ttk.Label(formula_frame, text=formula_text, font=("Courier", 10, "bold")).pack(pady=5)
        ttk.Label(formula_frame, text=explanation, font=("Helvetica", 9, "italic")).pack(pady=5)

        # Live Calculations (Right Column)
        calc_frame = ttk.LabelFrame(right_column, text="Live Calculations")
        calc_frame.pack(fill=tk.BOTH, expand=True, pady=5)
        
        calc_text = tk.Text(calc_frame, wrap=tk.WORD, height=20, width=50)
        calc_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        calc_text.insert(tk.END, calc_details)
        calc_text.config(state=tk.DISABLED)

def analyze_text():
    global last_analyzed_text, last_analyzed_scores
    text = text_entry.get("1.0", tk.END).strip()
    if not text:
        messagebox.showwarning("Input Error", "Please enter some text to analyze.")
        return

    # Analyze sentiment
    scores = analyzer.analyze_sentiment(text)
    final_sentiment, confidence = analyzer.get_final_sentiment(scores['final'])

    # Display results in respective frames
    ngram_result_text = "N-gram Analysis:\n"
    for sentiment, score in scores['ngram'].items():
        ngram_result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
    ngram_result_label.config(text=ngram_result_text)

    nb_result_text = "Naive Bayes Analysis:\n"
    for sentiment, score in scores['naive_bayes'].items():
        nb_result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
    nb_result_label.config(text=nb_result_text)

    svm_result_text = "SVM Analysis:\n"
    for sentiment, score in scores['svm'].items():
        svm_result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
    svm_result_label.config(text=svm_result_text)

    final_result_text = f"Final Consensus:\n"
    for sentiment, score in scores['final'].items():
        final_result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
    final_result_text += f"\nFinal Sentiment: {final_sentiment.capitalize()} (Confidence: {confidence:.2f}%)"
    final_result_label.config(text=final_result_text)

    # Store the last analyzed text and scores
    last_analyzed_text = text
    last_analyzed_scores = scores

    # Update Numbers tab visualizations
    numbers_tab = None
    for child in tab_control.winfo_children():
        if tab_control.tab(child)['text'] == 'Numbers':
            numbers_tab = child
            break

    if numbers_tab and hasattr(numbers_tab, 'update_viz'):
        numbers_tab.update_viz()  # This will update all tabs now

    # Clear the text box
    text_entry.delete("1.0", tk.END)

def update_numbers_tab(tab, text, scores):
    # Find the notebook within the Numbers tab
    numbers_notebook = None
    for child in tab.winfo_children():
        if isinstance(child, ttk.Notebook):
            numbers_notebook = child
            break
    
    if not numbers_notebook:
        return

    # Get the selected tab widget instead of just the tab name
    current_tab_id = numbers_notebook.select()
    if current_tab_id:  # Make sure a tab is actually selected
        # Get the actual widget for the selected tab
        current_tab = numbers_notebook.nametowidget(current_tab_id)
        tab_name = numbers_notebook.tab(current_tab_id)['text']
        
        if tab_name == "N-gram Details":
            update_ngram_details(current_tab, text, scores)
        elif tab_name == "Naive Bayes Details":
            update_nb_details(current_tab, text, scores)
        elif tab_name == "SVM Details":
            update_svm_details(current_tab, text, scores)

def update_ngram_details(tab, text, scores):
    # Clear existing content
    for widget in tab.winfo_children():
        widget.destroy()

    # Create figure
    fig = Figure(figsize=(10, 6))
    ax = fig.add_subplot(111)

    # Plot N-gram scores
    sentiments = list(scores['ngram'].keys())
    values = [scores['ngram'][s] for s in sentiments]
    x = np.arange(len(sentiments))
    
    bars = ax.bar(x, values)
    ax.set_xticks(x)
    ax.set_xticklabels(sentiments)
    ax.set_title('N-gram Analysis Scores')
    ax.set_ylabel('Probability (%)')

    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%', ha='center', va='bottom')

    canvas = FigureCanvasTkAgg(fig, master=tab)
    canvas.draw()
    canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)

    # Add detailed breakdown
    details_frame = ttk.LabelFrame(tab, text="Detailed Breakdown")
    details_frame.pack(fill=tk.X, padx=5, pady=5)
    
    bigrams = analyzer.get_ngrams(text, 2)
    trigrams = analyzer.get_ngrams(text, 3)
    
    ttk.Label(details_frame, text="Bigrams found:").pack(anchor=tk.W)
    for bigram in bigrams:
        ttk.Label(details_frame, text=f"• {bigram}").pack(anchor=tk.W)
    
    ttk.Label(details_frame, text="\nTrigrams found:").pack(anchor=tk.W)
    for trigram in trigrams:
        ttk.Label(details_frame, text=f"• {trigram}").pack(anchor=tk.W)

def update_nb_details(tab, text, scores):
    # Clear existing content
    for widget in tab.winfo_children():
        widget.destroy()

    # Create figure
    fig = Figure(figsize=(10, 6))
    ax = fig.add_subplot(111)

    # Plot Naive Bayes scores
    sentiments = list(scores['naive_bayes'].keys())
    values = [scores['naive_bayes'][s] for s in sentiments]
    x = np.arange(len(sentiments))
    
    bars = ax.bar(x, values)
    ax.set_xticks(x)
    ax.set_xticklabels(sentiments)
    ax.set_title('Naive Bayes Probability Distribution')
    ax.set_ylabel('Probability (%)')

    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%', ha='center', va='bottom')

    canvas = FigureCanvasTkAgg(fig, master=tab)
    canvas.draw()
    canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)

    # Add word probability breakdown
    details_frame = ttk.LabelFrame(tab, text="Word Probabilities")
    details_frame.pack(fill=tk.X, padx=5, pady=5)
    
    # Get word probabilities from the vectorizer
    words = analyzer.vectorizer.get_feature_names_out()
    for word in words:
        if word in text.lower():
            ttk.Label(details_frame, 
                     text=f"'{word}' probabilities:").pack(anchor=tk.W)
            for sentiment in sentiments:
                prob = analyzer.nb_model.feature_log_prob_[
                    list(analyzer.nb_model.classes_).index(sentiment)
                ][list(words).index(word)]
                ttk.Label(details_frame, 
                         text=f"  {sentiment}: {np.exp(prob):.4f}").pack(anchor=tk.W)

def update_svm_details(tab, text, scores):
    # Clear existing content
    for widget in tab.winfo_children():
        widget.destroy()

    # Create figure
    fig = Figure(figsize=(10, 6))
    ax = fig.add_subplot(111)

    # Plot SVM scores
    sentiments = list(scores['svm'].keys())
    values = [scores['svm'][s] for s in sentiments]
    x = np.arange(len(sentiments))
    
    bars = ax.bar(x, values)
    ax.set_xticks(x)
    ax.set_xticklabels(sentiments)
    ax.set_title('SVM Classification Probabilities')
    ax.set_ylabel('Probability (%)')

    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%', ha='center', va='bottom')

    canvas = FigureCanvasTkAgg(fig, master=tab)
    canvas.draw()
    canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)

    # Add SVM details
    details_frame = ttk.LabelFrame(tab, text="SVM Analysis Details")
    details_frame.pack(fill=tk.X, padx=5, pady=5)
    
    # Get decision function values
    X = analyzer.svm_model.named_steps['tfidfvectorizer'].transform([text])
    decision_values = analyzer.svm_model.named_steps['svc'].decision_function(X)
    
    ttk.Label(details_frame, text="Decision Function Values:").pack(anchor=tk.W)
    for i, sentiment in enumerate(sentiments):
        ttk.Label(details_frame, 
                 text=f"• {sentiment}: {decision_values[0][i]:.4f}").pack(anchor=tk.W)

def add_to_training():
    global last_analyzed_text
    if not last_analyzed_text:
        messagebox.showwarning("Input Error", "No text has been analyzed yet.")
        return

    sentiment = sentiment_var.get()
    if sentiment not in ['positive', 'neutral', 'negative']:
        messagebox.showwarning("Input Error", "Please select a sentiment.")
        return

    analyzer.add_to_training_data(last_analyzed_text, sentiment)
    messagebox.showinfo("Success", "Text added to training data successfully.")
    update_training_data_view()

def export_training_data():
    filepath = filedialog.asksaveasfilename(
        defaultextension=".csv",
        filetypes=[("CSV files", "*.csv")]
    )
    if filepath:
        analyzer.export_training_data(filepath)
        messagebox.showinfo("Success", "Training data exported successfully.")

def import_training_data():
    filepath = filedialog.askopenfilename(
        filetypes=[("CSV files", "*.csv")]
    )
    if filepath:
        analyzer.import_training_data(filepath)
        messagebox.showinfo("Success", "Training data imported successfully.")
        update_training_data_view()

def update_training_data_view():
    training_data = analyzer.get_training_data()
    training_data_text = "\n".join([f"{text} - {sentiment}" for text, sentiment in training_data])
    training_data_textbox.config(state=tk.NORMAL)
    training_data_textbox.delete("1.0", tk.END)
    training_data_textbox.insert(tk.END, training_data_text)
    training_data_textbox.config(state=tk.DISABLED)

def process_audio_file():
    global last_analyzed_text, last_analyzed_scores
    file_path = filedialog.askopenfilename(
        filetypes=[("Audio Files", "*.wav *.mp3 *.ogg *.m4a")]  # Allow more audio formats
    )
    if not file_path:
        return
    
    # Check if file exists
    if not Path(file_path).exists():
        messagebox.showerror("Error", f"File not found: {file_path}")
        audio_status_label.config(text="Error: File not found")
        return
        
    # Debug info
    print(f"Selected file: {file_path}")
    print(f"File exists: {Path(file_path).exists()}")
    print(f"File size: {Path(file_path).stat().st_size} bytes")
    
    audio_status_label.config(text=f"Processing audio file: {Path(file_path).name}")
    root.update()
    
    try:
        # Initialize recognizer with specific settings
        r = sr.Recognizer()
        r.energy_threshold = 4000
        r.dynamic_energy_threshold = True
        
        # Load the audio file
        try:
            with sr.AudioFile(file_path) as source:
                print("Audio file opened successfully")
                # Adjust for ambient noise
                r.adjust_for_ambient_noise(source)
                print("Ambient noise adjustment complete")
                audio = r.record(source)
                print("Audio recording complete")
        except Exception as e:
            print(f"Error opening audio file: {str(e)}")
            raise
            
        try:
            # Convert speech to text
            text = r.recognize_google(audio)
            
            # Display the transcribed text
            audio_text_display.config(state=tk.NORMAL)
            audio_text_display.delete(1.0, tk.END)
            audio_text_display.insert(tk.END, text)
            audio_text_display.config(state=tk.DISABLED)
            
            # Only proceed with sentiment analysis if we have text
            if text.strip():
                # Analyze sentiment
                scores = analyzer.analyze_sentiment(text)
                final_sentiment, confidence = analyzer.get_final_sentiment(scores['final'])
                
                # Store text for Numbers tab update
                last_analyzed_text = text
                last_analyzed_scores = scores

                # Display N-gram results
                ngram_text = "N-gram Analysis:\n"
                for sentiment, score in scores['ngram'].items():
                    ngram_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
                audio_ngram_result_label.config(text=ngram_text)

                # Display Naive Bayes results
                nb_text = "Naive Bayes Analysis:\n"
                for sentiment, score in scores['naive_bayes'].items():
                    nb_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
                audio_nb_result_label.config(text=nb_text)

                # Display SVM results
                svm_text = "SVM Analysis:\n"
                for sentiment, score in scores['svm'].items():
                    svm_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
                audio_svm_result_label.config(text=svm_text)

                # Display Final Consensus
                final_text = f"Final Consensus:\n"
                for sentiment, score in scores['final'].items():
                    final_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
                final_text += f"\nFinal Sentiment: {final_sentiment.capitalize()}\n(Confidence: {confidence:.2f}%)"
                audio_final_result_label.config(text=final_text)
                
                # Store the last analyzed text and scores
                last_analyzed_text = text
                last_analyzed_scores = scores
                
                # Update Numbers tab visualizations
                for child in tab_control.winfo_children():
                    if tab_control.tab(child)['text'] == 'Numbers':
                        numbers_tab = child
                        if hasattr(numbers_tab, 'update_viz'):
                            numbers_tab.update_viz()  # Use the existing update_viz method
                
                audio_status_label.config(text="Analysis complete!")
            else:
                audio_status_label.config(text="No speech detected in audio")
                
        except sr.UnknownValueError:
            messagebox.showerror("Error", "Could not understand the audio. Please ensure the audio is clear and contains speech.")
            audio_status_label.config(text="Error: Could not understand the audio")
        except sr.RequestError as e:
            messagebox.showerror("Error", f"Could not connect to the speech recognition service: {str(e)}")
            audio_status_label.config(text="Error: Service connection failed")
            
    except Exception as e:
        messagebox.showerror("Error", f"An error occurred while processing the audio file: {str(e)}")
        audio_status_label.config(text="Error: Processing failed")
        print(f"Detailed error: {str(e)}")  # For debugging

def add_to_training_from_audio():
    text = audio_text_display.get("1.0", tk.END).strip()
    if not text:
        messagebox.showwarning("Input Error", "No text has been transcribed yet.")
        return

    sentiment = audio_sentiment_var.get()
    if sentiment not in ['positive', 'neutral', 'negative']:
        messagebox.showwarning("Input Error", "Please select a sentiment.")
        return

    analyzer.add_to_training_data(text, sentiment)
    messagebox.showinfo("Success", "Text added to training data successfully.")
    update_training_data_view()

# Initialize analyzer and global variables
analyzer = SentimentAnalyzer()
last_analyzed_text = ""
last_analyzed_scores = None

# Create main window
root = tk.Tk()
root.title("Sentiment Analyzer")

# Create tab control
tab_control = ttk.Notebook(root)

# Create tabs
audio_tab = ttk.Frame(tab_control)
text_tab = ttk.Frame(tab_control)
numbers_tab = ttk.Frame(tab_control)
training_tab = ttk.Frame(tab_control)
about_tab = ttk.Frame(tab_control)

# Add tabs to control
tab_control.add(audio_tab, text="Audio Analysis")
tab_control.add(text_tab, text="Text Analysis")
tab_control.add(numbers_tab, text="Numbers")
tab_control.add(training_tab, text="Training Data")
tab_control.add(about_tab, text="About")
tab_control.pack(expand=1, fill="both")

# Initialize tabs
create_about_tab(about_tab)
create_numbers_tab(numbers_tab)

# Set up Text Analysis tab content
tk.Label(text_tab, text="Enter text to analyze:").pack(pady=10)
text_entry = tk.Text(text_tab, height=10, width=50)
text_entry.pack(pady=10)
analyze_button = tk.Button(text_tab, text="Analyze Sentiment", command=analyze_text)
analyze_button.pack(pady=10)

# Create a frame for the results
results_frame = tk.Frame(text_tab)
results_frame.pack(pady=10)

# N-gram Analysis
ngram_frame = tk.Frame(results_frame, borderwidth=1, relief="solid")
ngram_frame.pack(side=tk.LEFT, padx=5, pady=5)
tk.Label(ngram_frame, text="N-gram Analysis").pack()
ngram_result_label = tk.Label(ngram_frame, text="", justify=tk.LEFT)
ngram_result_label.pack()

# Naive Bayes Analysis
nb_frame = tk.Frame(results_frame, borderwidth=1, relief="solid")
nb_frame.pack(side=tk.LEFT, padx=5, pady=5)
tk.Label(nb_frame, text="Naive Bayes Analysis").pack()
nb_result_label = tk.Label(nb_frame, text="", justify=tk.LEFT)
nb_result_label.pack()

# SVM Analysis
svm_frame = tk.Frame(results_frame, borderwidth=1, relief="solid")
svm_frame.pack(side=tk.LEFT, padx=5, pady=5)
tk.Label(svm_frame, text="SVM Analysis").pack()
svm_result_label = tk.Label(svm_frame, text="", justify=tk.LEFT)
svm_result_label.pack()

# Final Consensus
final_frame = tk.Frame(text_tab, borderwidth=1, relief="solid")
final_frame.pack(pady=10)
tk.Label(final_frame, text="Final Consensus").pack()
final_result_label = tk.Label(final_frame, text="", justify=tk.LEFT)
final_result_label.pack()

# Define result_label to avoid the error
result_label = tk.Label(text_tab, text="", justify=tk.LEFT)
result_label.pack(pady=10)

# Create a frame for the buttons and sentiment selection
top_frame = tk.Frame(text_tab)
top_frame.pack(pady=10)

# Add text above the sentiment selection
tk.Label(top_frame, text="Add results to training data as:").pack(side=tk.TOP)

# Add sentiment selection for training data
sentiment_var = tk.StringVar(value="neutral")
tk.Radiobutton(top_frame, text="Positive", variable=sentiment_var, value="positive").pack(side=tk.TOP)
tk.Radiobutton(top_frame, text="Neutral", variable=sentiment_var, value="neutral").pack(side=tk.TOP)
tk.Radiobutton(top_frame, text="Negative", variable=sentiment_var, value="negative").pack(side=tk.TOP)

add_button = tk.Button(top_frame, text="Add results", command=add_to_training)
add_button.pack(side=tk.TOP, pady=5)

# Create a frame for the import and export buttons at the bottom
bottom_frame = tk.Frame(text_tab)
bottom_frame.pack(side=tk.BOTTOM, pady=10)

import_export_frame = tk.Frame(bottom_frame)
import_export_frame.pack(side=tk.TOP, pady=5)

export_button = tk.Button(import_export_frame, text="Export Training Data", command=export_training_data)
export_button.pack(side=tk.LEFT, padx=5)

import_button = tk.Button(import_export_frame, text="Import Training Data", command=import_training_data)
import_button.pack(side=tk.LEFT, padx=5)

# Set up Training Data tab content
training_data_textbox = tk.Text(training_tab, height=40, width=80, state=tk.DISABLED)
training_data_textbox.pack(pady=10)

# Update the training data view initially
update_training_data_view()

# Set up Audio Analysis tab content
tk.Label(audio_tab, text="Select an audio file for analysis:").pack(pady=10)
audio_button = tk.Button(audio_tab, text="Choose Audio File", command=process_audio_file)
audio_button.pack(pady=10)

audio_status_label = tk.Label(audio_tab, text="No file selected")
audio_status_label.pack(pady=5)

tk.Label(audio_tab, text="Transcribed Text:").pack(pady=5)
audio_text_display = tk.Text(audio_tab, height=8, width=50, state=tk.DISABLED)
audio_text_display.pack(pady=10)

# Create a frame for the audio analysis results
audio_results_frame = tk.Frame(audio_tab)
audio_results_frame.pack(pady=10)

# N-gram Analysis
audio_ngram_frame = tk.Frame(audio_results_frame, borderwidth=1, relief="solid")
audio_ngram_frame.pack(side=tk.LEFT, padx=5, pady=5)
tk.Label(audio_ngram_frame, text="N-gram Analysis").pack()
audio_ngram_result_label = tk.Label(audio_ngram_frame, text="", justify=tk.LEFT)
audio_ngram_result_label.pack()

# Naive Bayes Analysis
audio_nb_frame = tk.Frame(audio_results_frame, borderwidth=1, relief="solid")
audio_nb_frame.pack(side=tk.LEFT, padx=5, pady=5)
tk.Label(audio_nb_frame, text="Naive Bayes Analysis").pack()
audio_nb_result_label = tk.Label(audio_nb_frame, text="", justify=tk.LEFT)
audio_nb_result_label.pack()

# SVM Analysis
audio_svm_frame = tk.Frame(audio_results_frame, borderwidth=1, relief="solid")
audio_svm_frame.pack(side=tk.LEFT, padx=5, pady=5)
tk.Label(audio_svm_frame, text="SVM Analysis").pack()
audio_svm_result_label = tk.Label(audio_svm_frame, text="", justify=tk.LEFT)
audio_svm_result_label.pack()

# Final Consensus
audio_final_frame = tk.Frame(audio_tab, borderwidth=1, relief="solid")
audio_final_frame.pack(pady=10)
tk.Label(audio_final_frame, text="Final Consensus").pack()
audio_final_result_label = tk.Label(audio_final_frame, text="", justify=tk.LEFT)
audio_final_result_label.pack()

# Add training controls to Audio tab
audio_top_frame = tk.Frame(audio_tab)
audio_top_frame.pack(pady=10)

# Add text above the sentiment selection
tk.Label(audio_top_frame, text="Add results to training data as:").pack(side=tk.TOP)

# Add sentiment selection for training data
audio_sentiment_var = tk.StringVar(value="neutral")
tk.Radiobutton(audio_top_frame, text="Positive", variable=audio_sentiment_var, value="positive").pack(side=tk.TOP)
tk.Radiobutton(audio_top_frame, text="Neutral", variable=audio_sentiment_var, value="neutral").pack(side=tk.TOP)
tk.Radiobutton(audio_top_frame, text="Negative", variable=audio_sentiment_var, value="negative").pack(side=tk.TOP)

audio_add_button = tk.Button(audio_top_frame, text="Add results", command=lambda: add_to_training_from_audio())
audio_add_button.pack(side=tk.TOP, pady=5)

# Create a frame for the import and export buttons at the bottom
audio_bottom_frame = tk.Frame(audio_tab)
audio_bottom_frame.pack(side=tk.BOTTOM, pady=10)

audio_import_export_frame = tk.Frame(audio_bottom_frame)
audio_import_export_frame.pack(side=tk.TOP, pady=5)

audio_export_button = tk.Button(audio_import_export_frame, text="Export Training Data", command=export_training_data)
audio_export_button.pack(side=tk.LEFT, padx=5)

audio_import_button = tk.Button(audio_import_export_frame, text="Import Training Data", command=import_training_data)
audio_import_button.pack(side=tk.LEFT, padx=5)

# Start the main event loop
root.mainloop()