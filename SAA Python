import re
from collections import defaultdict
from typing import List, Dict, Tuple
import tkinter as tk
from tkinter import messagebox, filedialog
from tkinter import ttk
import speech_recognition as sr
from pathlib import Path
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import numpy as np
from matplotlib.figure import Figure
import math
import csv
import queue  # Add new import
import threading  # Add new import

class SentimentAnalyzer:
    def __init__(self):
        self.sentiment_data = {
            'positive': [],
            'neutral': [],
            'negative': []
        }
        self.ngram_counts = {
            'bigram': defaultdict(lambda: defaultdict(int)),
            'trigram': defaultdict(lambda: defaultdict(int))
        }

        # Initialize Naive Bayes and SVM models
        self.vectorizer = TfidfVectorizer()
        self.nb_model = MultinomialNB()
        self.svm_model = make_pipeline(TfidfVectorizer(), SVC(probability=True))

        self.load_training_data()
        self.train()

    def load_training_data(self):
        # Load training data in CSV format
        training_data = [
            ["Sentence", "Sentiment"],  # Header row
            ["I absolutely love this product!", "positive"],
            ["This is the best day ever!", "positive"],
            ["Great service and amazing quality", "positive"],
            ["The weather is okay today", "neutral"],
            ["I'm not sure how I feel about this", "neutral"],
            ["It could be better but it's not terrible", "neutral"],
            ["This is terrible, I hate it", "negative"],
            ["Worst experience ever, very disappointing", "negative"],
            ["Poor quality and bad service", "negative"],
            ["I love my phone", "positive"],
            ["I hate my phone", "negative"],
            ["My phone is okay", "neutral"],
            ["I am very happy with my purchase", "positive"],
            ["I am very unhappy with my purchase", "negative"],
            ["The product is average", "neutral"]
        ]

        # Skip header row and process data
        for text, sentiment in training_data[1:]:
            self.sentiment_data[sentiment.lower()].append(text.lower())

    def preprocess_text(self, text: str) -> str:
        # Clean and preprocess text
        text = text.lower()
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        return text

    def get_ngrams(self, text: str, n: int) -> List[str]:
        # Generate n-grams from text
        words = self.preprocess_text(text).split()
        ngrams = []
        for i in range(len(words) - n + 1):
            ngram = ' '.join(words[i:i + n])
            ngrams.append(ngram)
        return ngrams

    def train(self):
        # Train the N-gram model
        for sentiment, texts in self.sentiment_data.items():
            for text in texts:
                # Process bigrams
                bigrams = self.get_ngrams(text, 2)
                for bigram in bigrams:
                    self.ngram_counts['bigram'][sentiment][bigram] += 1

                # Process trigrams
                trigrams = self.get_ngrams(text, 3)
                for trigram in trigrams:
                    self.ngram_counts['trigram'][sentiment][trigram] += 1

        # Train Naive Bayes and SVM models
        all_texts = []
        all_labels = []
        for sentiment, texts in self.sentiment_data.items():
            all_texts.extend(texts)
            all_labels.extend([sentiment] * len(texts))
        self.nb_model.fit(self.vectorizer.fit_transform(all_texts), all_labels)
        self.svm_model.fit(all_texts, all_labels)

    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        # Analyze sentiment using N-gram method
        bigrams = self.get_ngrams(text, 2)
        trigrams = self.get_ngrams(text, 3)
        
        scores_ngram = {
            'positive': 0.0,
            'neutral': 0.0,
            'negative': 0.0
        }

        bigram_weight = 0.75
        trigram_weight = 0.25

        for bigram in bigrams:
            for sentiment in scores_ngram.keys():
                if bigram in self.ngram_counts['bigram'][sentiment]:
                    scores_ngram[sentiment] += self.ngram_counts['bigram'][sentiment][bigram] * bigram_weight

        for trigram in trigrams:
            for sentiment in scores_ngram.keys():
                if trigram in self.ngram_counts['trigram'][sentiment]:
                    scores_ngram[sentiment] += self.ngram_counts['trigram'][sentiment][trigram] * trigram_weight

        if sum(scores_ngram.values()) == 0:
            scores_ngram['neutral'] = 1.0

        total_score_ngram = sum(scores_ngram.values()) or 1
        normalized_scores_ngram = {
            sentiment: (score / total_score_ngram) * 100 
            for sentiment, score in scores_ngram.items()
        }

        # Analyze sentiment using Naive Bayes method
        nb_probs = self.nb_model.predict_proba(self.vectorizer.transform([text]))[0]
        scores_nb = {
            'positive': nb_probs[self.nb_model.classes_.tolist().index('positive')] * 100,
            'neutral': nb_probs[self.nb_model.classes_.tolist().index('neutral')] * 100,
            'negative': nb_probs[self.nb_model.classes_.tolist().index('negative')] * 100
        }

        # Analyze sentiment using SVM method
        svm_probs = self.svm_model.predict_proba([text])[0]
        scores_svm = {
            'positive': svm_probs[self.svm_model.classes_.tolist().index('positive')] * 100,
            'neutral': svm_probs[self.svm_model.classes_.tolist().index('neutral')] * 100,
            'negative': svm_probs[self.svm_model.classes_.tolist().index('negative')] * 100
        }

        # Average the results for final consensus
        final_scores = {
            sentiment: (normalized_scores_ngram[sentiment] + scores_nb[sentiment] + scores_svm[sentiment]) / 3
            for sentiment in scores_ngram.keys()
        }

        return {
            'ngram': normalized_scores_ngram,
            'naive_bayes': scores_nb,
            'svm': scores_svm,
            'final': final_scores
        }

    def get_final_sentiment(self, scores: Dict[str, float]) -> Tuple[str, float]:
        final_sentiment = max(scores.items(), key=lambda x: x[1])
        return final_sentiment

    def add_to_training_data(self, text: str, sentiment: str):
        # Add new text to training data and retrain the model
        self.sentiment_data[sentiment].append(text.lower())
        self.train()

    def export_training_data(self, filepath: str):
        # Export training data to a CSV file
        with open(filepath, 'w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(["Sentence", "Sentiment"])  # Write header
            for sentiment, texts in self.sentiment_data.items():
                for text in texts:
                    writer.writerow([text, sentiment])

    def import_training_data(self, filepath: str):
        if not Path(filepath).exists():
            messagebox.showerror("Error", f"File not found: {filepath}")
            return
        
        # Create progress window
        progress_window = tk.Toplevel()
        progress_window.title("Importing Data")
        progress_window.geometry("300x150")
        progress_window.transient(root)  # Make window modal
        progress_window.grab_set()  # Make window modal
        
        progress_label = ttk.Label(progress_window, text="Reading CSV file...")
        progress_label.pack(pady=10)
        
        # Create frame for progress bar and percentage
        progress_frame = ttk.Frame(progress_window)
        progress_frame.pack(pady=10)
        
        progress_bar = ttk.Progressbar(progress_frame, length=200, mode='determinate')
        progress_bar.pack(side=tk.LEFT, padx=(0, 5))
        
        percentage_label = ttk.Label(progress_frame, text="0%")
        percentage_label.pack(side=tk.LEFT)
        
        try:
            # First count total lines for progress tracking
            with open(filepath, 'r', encoding='utf-8-sig') as file:  # Use utf-8-sig to handle BOM
                total_lines = sum(1 for _ in file) - 1  # Subtract header
            progress_bar['maximum'] = total_lines
            
            # Process in batches
            BATCH_SIZE = 1000
            current_batch = []
            processed_lines = 0
            
            with open(filepath, 'r', encoding='utf-8-sig') as file:  # Use utf-8-sig to handle BOM
                reader = csv.DictReader(file)
                
                for row in reader:
                    sentiment = row['Sentiment'].lower()
                    text = row['Sentence'].lower()
                    
                    if sentiment in self.sentiment_data:
                        current_batch.append((text, sentiment))
                        processed_lines += 1
                        
                        if len(current_batch) >= BATCH_SIZE:
                            # Process batch
                            for text, sent in current_batch:
                                self.sentiment_data[sent].append(text)
                            current_batch = []
                            
                            # Update progress
                            progress_bar['value'] = processed_lines
                            percentage = min(100, int((processed_lines / total_lines) * 100))
                            percentage_label.config(text=f"{percentage}%")
                            progress_label.config(text=f"Processed {processed_lines:,} of {total_lines:,} entries")
                            progress_window.update()
                
                # Process remaining items
                for text, sent in current_batch:
                    self.sentiment_data[sent].append(text)
                
                # Ensure progress bar shows 100%
                progress_bar['value'] = total_lines
                percentage_label.config(text="100%")
                progress_label.config(text=f"Processed {total_lines:,} of {total_lines:,} entries")
                progress_window.update()
            
            progress_label.config(text="Training model...")
            progress_window.update()
            
            def cleanup():
                try:
                    root.after(1, progress_window.destroy)  # Schedule window destruction
                    root.after(200, update_training_data_view)  # Update the view after cleanup
                except:
                    pass
            
            # Train in a separate thread to prevent UI freeze
            def train_thread():
                try:
                    self.train()
                    root.after(1, cleanup)  # Schedule cleanup on main thread using root instead of progress_window
                except Exception as e:
                    root.after(1, progress_window.destroy)
                    root.after(100, lambda: messagebox.showerror("Error", f"Error training model: {str(e)}"))
            
            training_thread = threading.Thread(target=train_thread, daemon=True)
            training_thread.start()
            
        except Exception as e:
            progress_window.destroy()
            messagebox.showerror("Error", f"Error reading CSV file: {str(e)}")
            return

    def get_training_data(self) -> List[Tuple[str, str]]:
        # Get the training data as a list of tuples
        training_data = []
        for sentiment, texts in self.sentiment_data.items():
            for text in texts:
                training_data.append((text, sentiment))
        return training_data

def analyze_text():
    global last_analyzed_text, last_analyzed_scores
    text = text_entry.get("1.0", tk.END).strip()
    if not text:
        messagebox.showwarning("Input Error", "Please enter some text to analyze.")
        return

    # Analyze sentiment
    scores = analyzer.analyze_sentiment(text)
    final_sentiment, confidence = analyzer.get_final_sentiment(scores['final'])

    # Display results in respective frames
    ngram_result_text = "N-gram Analysis:\n"
    for sentiment, score in scores['ngram'].items():
        ngram_result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
    ngram_result_label.config(text=ngram_result_text)

    nb_result_text = "Naive Bayes Analysis:\n"
    for sentiment, score in scores['naive_bayes'].items():
        nb_result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
    nb_result_label.config(text=nb_result_text)

    svm_result_text = "SVM Analysis:\n"
    for sentiment, score in scores['svm'].items():
        svm_result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
    svm_result_label.config(text=svm_result_text)

    final_result_text = f"Final Consensus:\n"
    for sentiment, score in scores['final'].items():
        final_result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
    final_result_text += f"\nFinal Sentiment: {final_sentiment.capitalize()} (Confidence: {confidence:.2f}%)"
    final_result_label.config(text=final_result_text)

    # Store the last analyzed text and scores
    last_analyzed_text = text
    last_analyzed_scores = scores

    # Update Numbers tab visualizations
    numbers_tab = None
    for child in tab_control.winfo_children():
        if tab_control.tab(child)['text'] == 'Numbers':
            numbers_tab = child
            break

    if numbers_tab and hasattr(numbers_tab, 'update_viz'):
        numbers_tab.update_viz()

    # Clear the text box
    text_entry.delete("1.0", tk.END)

def update_numbers_tab(tab, text, scores):
    # Find the notebook within the Numbers tab
    numbers_notebook = None
    for child in tab.winfo_children():
        if isinstance(child, ttk.Notebook):
            numbers_notebook = child
            break
    
    if not numbers_notebook:
        return

    # Get the selected tab widget instead of just the tab name
    current_tab_id = numbers_notebook.select()
    if current_tab_id:  # Make sure a tab is actually selected
        # Get the actual widget for the selected tab
        current_tab = numbers_notebook.nametowidget(current_tab_id)
        tab_name = numbers_notebook.tab(current_tab_id)['text']
        
        if tab_name == "N-gram Details":
            update_ngram_details(current_tab, text, scores)
        elif tab_name == "Naive Bayes Details":
            update_nb_details(current_tab, text, scores)
        elif tab_name == "SVM Details":
            update_svm_details(current_tab, text, scores)

def update_ngram_details(tab, text, scores):
    # Clear existing content
    for widget in tab.winfo_children():
        widget.destroy()

    # Create figure
    fig = Figure(figsize=(10, 6))
    ax = fig.add_subplot(111)

    # Plot N-gram scores
    sentiments = list(scores['ngram'].keys())
    values = [scores['ngram'][s] for s in sentiments]
    x = np.arange(len(sentiments))
    
    bars = ax.bar(x, values)
    ax.set_xticks(x)
    ax.set_xticklabels(sentiments)
    ax.set_title('N-gram Analysis Scores')
    ax.set_ylabel('Probability (%)')

    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%', ha='center', va='bottom')

    canvas = FigureCanvasTkAgg(fig, master=tab)
    canvas.draw()
    canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)

    # Add detailed breakdown
    details_frame = ttk.LabelFrame(tab, text="Detailed Breakdown")
    details_frame.pack(fill=tk.X, padx=5, pady=5)
    
    bigrams = analyzer.get_ngrams(text, 2)
    trigrams = analyzer.get_ngrams(text, 3)
    
    ttk.Label(details_frame, text="Bigrams found:").pack(anchor=tk.W)
    for bigram in bigrams:
        ttk.Label(details_frame, text=f"• {bigram}").pack(anchor=tk.W)
    
    ttk.Label(details_frame, text="\nTrigrams found:").pack(anchor=tk.W)
    for trigram in trigrams:
        ttk.Label(details_frame, text=f"• {trigram}").pack(anchor=tk.W)

def update_nb_details(tab, text, scores):
    # Clear existing content
    for widget in tab.winfo_children():
        widget.destroy()

    # Create figure
    fig = Figure(figsize=(10, 6))
    ax = fig.add_subplot(111)

    # Plot Naive Bayes scores
    sentiments = list(scores['naive_bayes'].keys())
    values = [scores['naive_bayes'][s] for s in sentiments]
    x = np.arange(len(sentiments))
    
    bars = ax.bar(x, values)
    ax.set_xticks(x)
    ax.set_xticklabels(sentiments)
    ax.set_title('Naive Bayes Probability Distribution')
    ax.set_ylabel('Probability (%)')

    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%', ha='center', va='bottom')

    canvas = FigureCanvasTkAgg(fig, master=tab)
    canvas.draw()
    canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)

    # Add word probability breakdown
    details_frame = ttk.LabelFrame(tab, text="Word Probabilities")
    details_frame.pack(fill=tk.X, padx=5, pady=5)
    
    # Get word probabilities from the vectorizer
    words = analyzer.vectorizer.get_feature_names_out()
    for word in words:
        if word in text.lower():
            ttk.Label(details_frame, 
                     text=f"'{word}' probabilities:").pack(anchor=tk.W)
            for sentiment in sentiments:
                prob = analyzer.nb_model.feature_log_prob_[
                    list(analyzer.nb_model.classes_).index(sentiment)
                ][list(words).index(word)]
                ttk.Label(details_frame, 
                         text=f"  {sentiment}: {np.exp(prob):.4f}").pack(anchor=tk.W)

def update_svm_details(tab, text, scores):
    # Clear existing content
    for widget in tab.winfo_children():
        widget.destroy()

    # Create figure
    fig = Figure(figsize=(10, 6))
    ax = fig.add_subplot(111)

    # Plot SVM scores
    sentiments = list(scores['svm'].keys())
    values = [scores['svm'][s] for s in sentiments]
    x = np.arange(len(sentiments))
    
    bars = ax.bar(x, values)
    ax.set_xticks(x)
    ax.set_xticklabels(sentiments)
    ax.set_title('SVM Classification Probabilities')
    ax.set_ylabel('Probability (%)')

    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%', ha='center', va='bottom')

    canvas = FigureCanvasTkAgg(fig, master=tab)
    canvas.draw()
    canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)

    # Add SVM details
    details_frame = ttk.LabelFrame(tab, text="SVM Analysis Details")
    details_frame.pack(fill=tk.X, padx=5, pady=5)
    
    # Get decision function values
    X = analyzer.svm_model.named_steps['tfidfvectorizer'].transform([text])
    decision_values = analyzer.svm_model.named_steps['svc'].decision_function(X)
    
    ttk.Label(details_frame, text="Decision Function Values:").pack(anchor=tk.W)
    for i, sentiment in enumerate(sentiments):
        ttk.Label(details_frame, 
                 text=f"• {sentiment}: {decision_values[0][i]:.4f}").pack(anchor=tk.W)

def add_to_training():
    global last_analyzed_text
    if not last_analyzed_text:
        messagebox.showwarning("Input Error", "No text has been analyzed yet.")
        return

    sentiment = sentiment_var.get()
    if sentiment not in ['positive', 'neutral', 'negative']:
        messagebox.showwarning("Input Error", "Please select a sentiment.")
        return

    analyzer.add_to_training_data(last_analyzed_text, sentiment)
    messagebox.showinfo("Success", "Text added to training data successfully.")
    update_training_data_view()

def export_training_data():
    filepath = filedialog.asksaveasfilename(
        defaultextension=".csv",
        filetypes=[("CSV files", "*.csv")]
    )
    if filepath:
        analyzer.export_training_data(filepath)
        messagebox.showinfo("Success", "Training data exported successfully.")

def import_training_data():
    filepath = filedialog.askopenfilename(
        filetypes=[("CSV files", "*.csv")]
    )
    if filepath:
        analyzer.import_training_data(filepath)
        messagebox.showinfo("Success", "Training data imported successfully.")
        update_training_data_view()

def update_training_data_view():
    training_data = analyzer.get_training_data()
    training_data_text = "\n".join([f"{text} - {sentiment}" for text, sentiment in training_data])
    training_data_textbox.config(state=tk.NORMAL)
    training_data_textbox.delete("1.0", tk.END)
    training_data_textbox.insert(tk.END, training_data_text)
    training_data_textbox.config(state=tk.DISABLED)

# Initialize analyzer
analyzer = SentimentAnalyzer()
last_analyzed_text = ""
last_analyzed_scores = None  # Store the last analysis scores

# Create the main window
root = tk.Tk()
root.title("Sentiment Analyzer")

# Create a tabbed interface
tab_control = ttk.Notebook(root)
audio_tab = ttk.Frame(tab_control)      # Tab 1: Audio Analysis
text_tab = ttk.Frame(tab_control)       # Tab 2: Text Analysis
numbers_tab = ttk.Frame(tab_control)    # Tab 3: Numbers
training_tab = ttk.Frame(tab_control)   # Tab 4: Training Data
about_tab = ttk.Frame(tab_control)      # Tab 5: About

# Add tabs in the correct order
tab_control.add(audio_tab, text="Audio Analysis")
tab_control.add(text_tab, text="Text Analysis")
tab_control.add(numbers_tab, text="Numbers")
tab_control.add(training_tab, text="Training Data")
tab_control.add(about_tab, text="About")
tab_control.pack(expand=1, fill="both")

# Tab 1: Sentiment Analysis
tk.Label(text_tab, text="Enter text to analyze:").pack(pady=10)
text_entry = tk.Text(text_tab, height=10, width=50)
text_entry.pack(pady=10)
analyze_button = tk.Button(text_tab, text="Analyze Sentiment", command=analyze_text)
analyze_button.pack(pady=10)

# Create a frame for the results
results_frame = tk.Frame(text_tab)
results_frame.pack(pady=10)

# N-gram Analysis
ngram_frame = tk.Frame(results_frame, borderwidth=1, relief="solid")
ngram_frame.pack(side=tk.LEFT, padx=5, pady=5)
tk.Label(ngram_frame, text="N-gram Analysis").pack()
ngram_result_label = tk.Label(ngram_frame, text="", justify=tk.LEFT)
ngram_result_label.pack()

# Naive Bayes Analysis
nb_frame = tk.Frame(results_frame, borderwidth=1, relief="solid")
nb_frame.pack(side=tk.LEFT, padx=5, pady=5)
tk.Label(nb_frame, text="Naive Bayes Analysis").pack()
nb_result_label = tk.Label(nb_frame, text="", justify=tk.LEFT)
nb_result_label.pack()

# SVM Analysis
svm_frame = tk.Frame(results_frame, borderwidth=1, relief="solid")
svm_frame.pack(side=tk.LEFT, padx=5, pady=5)
tk.Label(svm_frame, text="SVM Analysis").pack()
svm_result_label = tk.Label(svm_frame, text="", justify=tk.LEFT)
svm_result_label.pack()

# Final Consensus
final_frame = tk.Frame(text_tab, borderwidth=1, relief="solid")
final_frame.pack(pady=10)
tk.Label(final_frame, text="Final Consensus").pack()
final_result_label = tk.Label(final_frame, text="", justify=tk.LEFT)
final_result_label.pack()

# Define result_label to avoid the error
result_label = tk.Label(text_tab, text="", justify=tk.LEFT)
result_label.pack(pady=10)

# Create a frame for the buttons and sentiment selection
top_frame = tk.Frame(text_tab)
top_frame.pack(pady=10)

# Add text above the sentiment selection
tk.Label(top_frame, text="Add results to training data as:").pack(side=tk.TOP)

# Add sentiment selection for training data
sentiment_var = tk.StringVar(value="neutral")
tk.Radiobutton(top_frame, text="Positive", variable=sentiment_var, value="positive").pack(side=tk.TOP)
tk.Radiobutton(top_frame, text="Neutral", variable=sentiment_var, value="neutral").pack(side=tk.TOP)
tk.Radiobutton(top_frame, text="Negative", variable=sentiment_var, value="negative").pack(side=tk.TOP)

add_button = tk.Button(top_frame, text="Add results", command=add_to_training)
add_button.pack(side=tk.TOP, pady=5)

# Create a frame for the import and export buttons at the bottom
bottom_frame = tk.Frame(text_tab)
bottom_frame.pack(side=tk.BOTTOM, pady=10)

import_export_frame = tk.Frame(bottom_frame)
import_export_frame.pack(side=tk.TOP, pady=5)

export_button = tk.Button(import_export_frame, text="Export Training Data", command=export_training_data)
export_button.pack(side=tk.LEFT, padx=5)

import_button = tk.Button(import_export_frame, text="Import Training Data", command=import_training_data)
import_button.pack(side=tk.LEFT, padx=5)

# Tab 2: Training Data
training_data_textbox = tk.Text(training_tab, height=40, width=80, state=tk.DISABLED)
training_data_textbox.pack(pady=10)

# Update the training data view initially
update_training_data_view()

def process_audio_file():
    global last_analyzed_text, last_analyzed_scores
    file_path = filedialog.askopenfilename(
        filetypes=[("Audio Files", "*.wav *.mp3 *.ogg *.m4a")]  # Allow more audio formats
    )
    if not file_path:
        return
    
    # Check if file exists
    if not Path(file_path).exists():
        messagebox.showerror("Error", f"File not found: {file_path}")
        audio_status_label.config(text="Error: File not found")
        return
        
    # Debug info
    print(f"Selected file: {file_path}")
    print(f"File exists: {Path(file_path).exists()}")
    print(f"File size: {Path(file_path).stat().st_size} bytes")
    
    audio_status_label.config(text=f"Processing audio file: {Path(file_path).name}")
    root.update()
    
    try:
        # Initialize recognizer with specific settings
        r = sr.Recognizer()
        r.energy_threshold = 4000
        r.dynamic_energy_threshold = True
        
        # Load the audio file
        try:
            with sr.AudioFile(file_path) as source:
                print("Audio file opened successfully")
                # Adjust for ambient noise
                r.adjust_for_ambient_noise(source)
                print("Ambient noise adjustment complete")
                audio = r.record(source)
                print("Audio recording complete")
        except Exception as e:
            print(f"Error opening audio file: {str(e)}")
            raise
            
        try:
            # Convert speech to text
            text = r.recognize_google(audio)
            
            # Display the transcribed text
            audio_text_display.config(state=tk.NORMAL)
            audio_text_display.delete(1.0, tk.END)
            audio_text_display.insert(tk.END, text)
            audio_text_display.config(state=tk.DISABLED)
            
            # Only proceed with sentiment analysis if we have text
            if text.strip():
                # Analyze sentiment
                scores = analyzer.analyze_sentiment(text)
                final_sentiment, confidence = analyzer.get_final_sentiment(scores['final'])
                
                # Store text for Numbers tab update
                current_text = text

                # Display N-gram results
                ngram_text = "N-gram Analysis:\n"
                for sentiment, score in scores['ngram'].items():
                    ngram_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
                audio_ngram_result_label.config(text=ngram_text)

                # Display Naive Bayes results
                nb_text = "Naive Bayes Analysis:\n"
                for sentiment, score in scores['naive_bayes'].items():
                    nb_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
                audio_nb_result_label.config(text=nb_text)

                # Display SVM results
                svm_text = "SVM Analysis:\n"
                for sentiment, score in scores['svm'].items():
                    svm_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
                audio_svm_result_label.config(text=svm_text)

                # Display Final Consensus
                final_text = f"Final Consensus:\n"
                for sentiment, score in scores['final'].items():
                    final_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
                final_text += f"\nFinal Sentiment: {final_sentiment.capitalize()}\n(Confidence: {confidence:.2f}%)"
                audio_final_result_label.config(text=final_text)
                
                # Store the last analyzed text and scores
                last_analyzed_text = text
                last_analyzed_scores = scores
                
                # Update Numbers tab visualizations with the current text and scores
                for child in tab_control.winfo_children():
                    if tab_control.tab(child)['text'] == 'Numbers':
                        update_numbers_tab(child, current_text, scores)

                # Update visualizations in Numbers tab
                for child in tab_control.winfo_children():
                    if tab_control.tab(child)['text'] == 'Numbers':
                        for notebook in child.winfo_children():
                            if isinstance(notebook, ttk.Notebook):
                                for tab in notebook.winfo_children():
                                    for button in tab.winfo_children():
                                        if isinstance(button, ttk.Button):
                                            button.invoke()

                audio_status_label.config(text="Analysis complete!")
            else:
                audio_status_label.config(text="No speech detected in audio")
                
        except sr.UnknownValueError:
            messagebox.showerror("Error", "Could not understand the audio. Please ensure the audio is clear and contains speech.")
            audio_status_label.config(text="Error: Could not understand the audio")
        except sr.RequestError as e:
            messagebox.showerror("Error", f"Could not connect to the speech recognition service: {str(e)}")
            audio_status_label.config(text="Error: Service connection failed")
            
    except Exception as e:
        messagebox.showerror("Error", f"An error occurred while processing the audio file: {str(e)}")
        audio_status_label.config(text="Error: Processing failed")
        print(f"Detailed error: {str(e)}")  # For debugging

# Tab 3: Audio Analysis
tk.Label(audio_tab, text="Select an audio file for analysis:").pack(pady=10)
audio_button = tk.Button(audio_tab, text="Choose Audio File", command=process_audio_file)
audio_button.pack(pady=10)

audio_status_label = tk.Label(audio_tab, text="No file selected")
audio_status_label.pack(pady=5)

tk.Label(audio_tab, text="Transcribed Text:").pack(pady=5)
audio_text_display = tk.Text(audio_tab, height=8, width=50, state=tk.DISABLED)
audio_text_display.pack(pady=10)

# Create a frame for the audio analysis results
audio_results_frame = tk.Frame(audio_tab)
audio_results_frame.pack(pady=10)

# N-gram Analysis
audio_ngram_frame = tk.Frame(audio_results_frame, borderwidth=1, relief="solid")
audio_ngram_frame.pack(side=tk.LEFT, padx=5, pady=5)
tk.Label(audio_ngram_frame, text="N-gram Analysis").pack()
audio_ngram_result_label = tk.Label(audio_ngram_frame, text="", justify=tk.LEFT)
audio_ngram_result_label.pack()

# Naive Bayes Analysis
audio_nb_frame = tk.Frame(audio_results_frame, borderwidth=1, relief="solid")
audio_nb_frame.pack(side=tk.LEFT, padx=5, pady=5)
tk.Label(audio_nb_frame, text="Naive Bayes Analysis").pack()
audio_nb_result_label = tk.Label(audio_nb_frame, text="", justify=tk.LEFT)
audio_nb_result_label.pack()

# SVM Analysis
audio_svm_frame = tk.Frame(audio_results_frame, borderwidth=1, relief="solid")
audio_svm_frame.pack(side=tk.LEFT, padx=5, pady=5)
tk.Label(audio_svm_frame, text="SVM Analysis").pack()
audio_svm_result_label = tk.Label(audio_svm_frame, text="", justify=tk.LEFT)
audio_svm_result_label.pack()

# Final Consensus
audio_final_frame = tk.Frame(audio_tab, borderwidth=1, relief="solid")
audio_final_frame.pack(pady=10)
tk.Label(audio_final_frame, text="Final Consensus").pack()
audio_final_result_label = tk.Label(audio_final_frame, text="", justify=tk.LEFT)
audio_final_result_label.pack()

# Add training controls to Audio tab
audio_top_frame = tk.Frame(audio_tab)
audio_top_frame.pack(pady=10)

# Add text above the sentiment selection
tk.Label(audio_top_frame, text="Add results to training data as:").pack(side=tk.TOP)

# Add sentiment selection for training data
audio_sentiment_var = tk.StringVar(value="neutral")
tk.Radiobutton(audio_top_frame, text="Positive", variable=audio_sentiment_var, value="positive").pack(side=tk.TOP)
tk.Radiobutton(audio_top_frame, text="Neutral", variable=audio_sentiment_var, value="neutral").pack(side=tk.TOP)
tk.Radiobutton(audio_top_frame, text="Negative", variable=audio_sentiment_var, value="negative").pack(side=tk.TOP)

audio_add_button = tk.Button(audio_top_frame, text="Add results", command=lambda: add_to_training_from_audio())
audio_add_button.pack(side=tk.TOP, pady=5)

# Create a frame for the import and export buttons at the bottom
audio_bottom_frame = tk.Frame(audio_tab)
audio_bottom_frame.pack(side=tk.BOTTOM, pady=10)

audio_import_export_frame = tk.Frame(audio_bottom_frame)
audio_import_export_frame.pack(side=tk.TOP, pady=5)

audio_export_button = tk.Button(audio_import_export_frame, text="Export Training Data", command=export_training_data)
audio_export_button.pack(side=tk.LEFT, padx=5)

audio_import_button = tk.Button(audio_import_export_frame, text="Import Training Data", command=import_training_data)
audio_import_button.pack(side=tk.LEFT, padx=5)

# Add new function to handle adding audio transcription to training data
def add_to_training_from_audio():
    text = audio_text_display.get("1.0", tk.END).strip()
    if not text:
        messagebox.showwarning("Input Error", "No text has been transcribed yet.")
        return

    sentiment = audio_sentiment_var.get()
    if sentiment not in ['positive', 'neutral', 'negative']:
        messagebox.showwarning("Input Error", "Please select a sentiment.")
        return

    analyzer.add_to_training_data(text, sentiment)
    messagebox.showinfo("Success", "Text added to training data successfully.")
    update_training_data_view()

def create_about_tab(tab):
    # Create a scrollable frame
    canvas = tk.Canvas(tab)
    scrollbar = ttk.Scrollbar(tab, orient="vertical", command=canvas.yview)
    scrollable_frame = ttk.Frame(canvas)

    scrollable_frame.bind(
        "<Configure>",
        lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
    )

    canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
    canvas.configure(yscrollcommand=scrollbar.set)

    # Title
    title = ttk.Label(
        scrollable_frame,
        text="About the Sentiment Analyzer",
        font=("Helvetica", 14, "bold")
    )
    title.pack(pady=10)

    # What is Sentiment Analysis section
    what_is = ttk.LabelFrame(scrollable_frame, text="What is Sentiment Analysis?")
    ttk.Label(
        what_is,
        text="Sentiment analysis is the process of determining the emotional tone behind "
             "a piece of text. This tool combines three different approaches to sentiment analysis:",
        wraplength=500
    ).pack(pady=5)
    
    approaches = ttk.Frame(what_is)
    ttk.Label(approaches, text="• N-gram Analysis (phrase-based)").pack(anchor="w")
    ttk.Label(approaches, text="• Naive Bayes Classification (word-based)").pack(anchor="w")
    ttk.Label(approaches, text="• Support Vector Machine (SVM) Classification").pack(anchor="w")
    approaches.pack(pady=5)
    what_is.pack(fill="x", padx=10, pady=5)

    # N-gram Analysis section
    ngram = ttk.LabelFrame(scrollable_frame, text="N-gram Analysis")
    ttk.Label(
        ngram,
        text="N-gram analysis examines sequences of consecutive words to understand context and sentiment patterns. "
             "This method is particularly effective at capturing local context and common sentiment expressions:",
        wraplength=500
    ).pack(pady=5)
    
    formula = ttk.Label(
        ngram,
        text="Score = 0.75 × Bigram_Score + 0.25 × Trigram_Score",
        font=("Courier", 10, "bold")
    )
    formula.pack(pady=5)
    
    ngram_types = ttk.Frame(ngram)
    ttk.Label(ngram_types, text="• Bigrams capture two-word patterns (e.g., 'very good', 'not bad')").pack(anchor="w")
    ttk.Label(ngram_types, text="• Trigrams capture three-word patterns (e.g., 'not very good')").pack(anchor="w")
    ttk.Label(ngram_types, text="• Weights favor bigrams (75%) as they are more likely to have an accurate sentiment").pack(anchor="w")
    ttk.Label(ngram_types, text="• Scores are normalized across sentiment classes").pack(anchor="w")
    ngram_types.pack(pady=5)

    example_frame = ttk.Frame(ngram)
    ttk.Label(example_frame, 
              text="Example Analysis:",
              font=("Helvetica", 9, "bold")).pack(anchor="w")
    ttk.Label(example_frame, 
              text="Text: 'not very good'\n"
              "Bigrams: ['not very', 'very good']\n"
              "Trigrams: ['not very good']\n"
              "Final score combines bigram and trigram frequencies").pack(anchor="w")
    example_frame.pack(pady=5)
    ngram.pack(fill="x", padx=10, pady=5)

    # Naive Bayes Analysis section
    naive_bayes = ttk.LabelFrame(scrollable_frame, text="Naive Bayes Analysis")
    ttk.Label(
        naive_bayes,
        text="Naive Bayes is a probabilistic classifier that uses Bayes' Theorem to calculate the likelihood of a "
             "sentiment based on the presence of specific words. It's called 'naive' because it assumes word "
             "independence:",
        wraplength=500
    ).pack(pady=5)
    
    formula = ttk.Label(
        naive_bayes,
        text="P(sentiment|text) = P(sentiment) × ∏ P(word|sentiment)",
        font=("Courier", 10, "bold")
    )
    formula.pack(pady=5)
    
    nb_components = ttk.Frame(naive_bayes)
    ttk.Label(nb_components, text="• Prior Probability P(sentiment) - Base frequency of each sentiment").pack(anchor="w")
    ttk.Label(nb_components, text="• Likelihood P(word|sentiment) - Word frequency in each sentiment").pack(anchor="w")
    ttk.Label(nb_components, text="• Uses TF-IDF for word importance weighting").pack(anchor="w")
    ttk.Label(nb_components, text="• Applies Laplace smoothing for unseen words").pack(anchor="w")
    nb_components.pack(pady=5)

    example_frame = ttk.Frame(naive_bayes)
    ttk.Label(example_frame, 
              text="Example Analysis:",
              font=("Helvetica", 9, "bold")).pack(anchor="w")
    ttk.Label(example_frame, 
              text="Text: 'great product'\n"
              "1. Calculate P(positive) from training data\n"
              "2. Calculate P('great'|positive) and P('product'|positive)\n"
              "3. Multiply probabilities and normalize").pack(anchor="w")
    example_frame.pack(pady=5)
    naive_bayes.pack(fill="x", padx=10, pady=5)

    # SVM Analysis section
    svm = ttk.LabelFrame(scrollable_frame, text="Support Vector Machine Analysis")
    ttk.Label(
        svm,
        text="SVM is a powerful classifier that finds an optimal hyperplane to separate different sentiment classes "
             "in a high-dimensional space. It can handle complex, non-linear sentiment patterns through kernel functions:",
        wraplength=500
    ).pack(pady=5)
    
    formula = ttk.Label(
        svm,
        text="f(x) = Σ(αᵢ × K(xᵢ, x) + b)",
        font=("Courier", 10, "bold")
    )
    formula.pack(pady=5)
    
    svm_components = ttk.Frame(svm)
    ttk.Label(svm_components, text="• Kernel Function K(x,y) transforms text into feature space").pack(anchor="w")
    ttk.Label(svm_components, text="• Support Vectors are the most important training examples").pack(anchor="w")
    ttk.Label(svm_components, text="• Coefficients αᵢ determine the influence of each support vector").pack(anchor="w")
    ttk.Label(svm_components, text="• Bias term b adjusts the hyperplane position").pack(anchor="w")
    svm_components.pack(pady=5)

    example_frame = ttk.Frame(svm)
    ttk.Label(example_frame, 
              text="Example Analysis:",
              font=("Helvetica", 9, "bold")).pack(anchor="w")
    ttk.Label(example_frame, 
              text="Text: 'not good but great'\n"
              "1. Convert text to TF-IDF feature vector\n"
              "2. Apply kernel transformation\n"
              "3. Calculate distance from hyperplane\n"
              "4. Convert to probability using distance").pack(anchor="w")
    example_frame.pack(pady=5)
    svm.pack(fill="x", padx=10, pady=5)

    # Final Consensus section
    consensus = ttk.LabelFrame(scrollable_frame, text="Final Consensus")
    ttk.Label(
        consensus,
        text="The final sentiment combines all three methods to leverage their individual strengths:\n"
             "• N-grams capture local context and common phrases\n"
             "• Naive Bayes provides robust probabilistic classification\n"
             "• SVM handles complex, non-linear sentiment patterns",
        wraplength=500
    ).pack(pady=5)
    
    formula = ttk.Label(
        consensus,
        text="Final_Score = (N-gram_Score + NB_Score + SVM_Score) / 3",
        font=("Courier", 10, "bold")
    )
    formula.pack(pady=5)

    example_frame = ttk.Frame(consensus)
    ttk.Label(example_frame, 
              text="Example Combination:",
              font=("Helvetica", 9, "bold")).pack(anchor="w")
    ttk.Label(example_frame, 
              text="N-gram: 70% positive (captures phrases)\n"
              "Naive Bayes: 65% positive (word probabilities)\n"
              "SVM: 75% positive (non-linear patterns)\n"
              "Final Score: 70% positive (averaged)").pack(anchor="w")
    example_frame.pack(pady=5)
    consensus.pack(fill="x", padx=10, pady=5)

    # Key Features section
    features = ttk.LabelFrame(scrollable_frame, text="Key Features")
    features_list = [
        "• Dual analysis methods (Naive Bayes and N-grams)",
        "• Laplace (add-one) smoothing for both approaches",
        "• Log probabilities to prevent numerical underflow",
        "• Confidence scores using probability normalization",
        "• Interactive training data management",
        "• CSV import/export capabilities",
        "• Detailed probability calculations view"
    ]
    for feature in features_list:
        ttk.Label(features, text=feature).pack(anchor="w")
    features.pack(fill="x", padx=10, pady=5)

    # Text Processing section
    processing = ttk.LabelFrame(scrollable_frame, text="Text Processing")
    process_steps = [
        "1. Text is converted to lowercase",
        "2. Punctuation is removed",
        "3. Text is split into tokens (words or n-grams)",
        "4. Empty tokens are filtered out"
    ]
    for step in process_steps:
        ttk.Label(processing, text=step).pack(anchor="w")
    processing.pack(fill="x", padx=10, pady=5)

    # Training Process section
    training = ttk.LabelFrame(scrollable_frame, text="Training Process")
    train_steps = [
        "1. Count occurrences of tokens in each class",
        "2. Calculate prior probabilities for each class",
        "3. Apply Laplace smoothing to token probabilities",
        "4. Store model parameters for both analysis methods"
    ]
    for step in train_steps:
        ttk.Label(training, text=step).pack(anchor="w")
    training.pack(fill="x", padx=10, pady=5)

    # Prediction Process section
    prediction = ttk.LabelFrame(scrollable_frame, text="Prediction Process")
    pred_steps = [
        "1. Preprocess input text",
        "2. Generate appropriate tokens (words or n-grams)",
        "3. Calculate log probabilities for each class",
        "4. Sum log probabilities of tokens and prior",
        "5. Convert to normalized confidence scores",
        "6. Select class with highest probability"
    ]
    for step in pred_steps:
        ttk.Label(prediction, text=step).pack(anchor="w")
    prediction.pack(fill="x", padx=10, pady=5)

    # Pack the canvas and scrollbar
    canvas.pack(side="left", fill="both", expand=True, padx=5, pady=5)
    scrollbar.pack(side="right", fill="y")

# Modify your existing code to add the About tab
create_about_tab(about_tab)

def create_numbers_tab(tab):
    frames = {}  # Store frames in a dictionary to maintain references
    
    # Create a notebook for sub-tabs within Numbers tab
    numbers_notebook = ttk.Notebook(tab)
    
    # Create sub-tabs for each model
    ngram_tab = ttk.Frame(numbers_notebook)
    nb_tab = ttk.Frame(numbers_notebook)
    svm_tab = ttk.Frame(numbers_notebook)
    
    numbers_notebook.add(ngram_tab, text="N-gram Details")
    numbers_notebook.add(nb_tab, text="Naive Bayes Details")
    numbers_notebook.add(svm_tab, text="SVM Details")
    numbers_notebook.pack(expand=1, fill="both")

    # Create persistent frames
    frames['ngram'] = ttk.Frame(ngram_tab)
    frames['nb'] = ttk.Frame(nb_tab)
    frames['svm'] = ttk.Frame(svm_tab)

    for frame in frames.values():
        frame.pack(fill=tk.BOTH, expand=1)

    def update_viz():
        if not last_analyzed_text or not last_analyzed_scores:
            return

        try:
            current_tab = numbers_notebook.select()
            if not current_tab:
                return

            tab_name = numbers_notebook.tab(current_tab)['text']
            
            if tab_name == "N-gram Details":
                update_viz_content(frames['ngram'], 'ngram')
            elif tab_name == "Naive Bayes Details":
                update_viz_content(frames['nb'], 'naive_bayes')
            elif tab_name == "SVM Details":
                update_viz_content(frames['svm'], 'svm')
        except Exception as e:
            print(f"Error updating visualization: {str(e)}")

    # Store the update_viz function as an attribute of the tab
    tab.update_viz = update_viz

    def update_viz_content(frame, method):
        # Clear existing widgets
        for widget in frame.winfo_children():
            widget.destroy()

        # Add formula at the top
        if method == 'ngram':
            formula_text = "Score = 0.75 × Bigram_Score + 0.25 × Trigram_Score"
            explanation = "Weighted combination of bigram and trigram frequencies"
        elif method == 'naive_bayes':
            formula_text = "P(sentiment|text) = P(sentiment) × ∏ P(word|sentiment)"
            explanation = "Product of prior and word probabilities"
        else:  # SVM
            formula_text = "f(x) = Σ(αᵢ × K(xᵢ, x) + b)"
            explanation = "Where K is the kernel function and αᵢ are support vector coefficients"

        formula_label = ttk.Label(frame, text=formula_text, font=("Courier", 10, "bold"))
        formula_label.pack(pady=(10, 0))
        explanation_label = ttk.Label(frame, text=explanation, font=("Helvetica", 9, "italic"))
        explanation_label.pack(pady=(0, 10))

        # Create figure
        fig = Figure(figsize=(10, 6))
        ax = fig.add_subplot(111)

        # Plot scores
        sentiments = list(last_analyzed_scores[method].keys())
        values = [last_analyzed_scores[method][s] for s in sentiments]
        x = np.arange(len(sentiments))
        
        bars = ax.bar(x, values)
        ax.set_xticks(x)
        ax.set_xticklabels(sentiments)
        ax.set_title(f'{method.replace("_", " ").title()} Analysis Scores')
        ax.set_ylabel('Probability (%)')

        # Add value labels on bars
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.1f}%', ha='center', va='bottom')

        # Add the plot to the frame
        canvas = FigureCanvasTkAgg(fig, master=frame)
        canvas.draw()
        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

        # Add detailed explanation at the bottom
        formula_frame = ttk.LabelFrame(frame, text=f"Detailed {method.replace('_', ' ').title()} Explanation")
        formula_frame.pack(fill=tk.X, padx=5, pady=5)

        if method == 'ngram':
            detail_text = (
                "N-gram Analysis calculates sentiment scores by:\n"
                "1. Counting bigram (2-word) and trigram (3-word) frequencies\n"
                "2. Weighing bigrams at 75% and trigrams at 25%\n"
                "3. Normalizing scores across all sentiment classes"
            )
        elif method == 'naive_bayes':
            detail_text = (
                "Naive Bayes calculates sentiment probability by:\n"
                "1. Computing prior probability P(sentiment)\n"
                "2. Computing word likelihood P(word|sentiment)\n"
                "3. Multiplying probabilities and normalizing results"
            )
        else:  # SVM
            detail_text = (
                "Support Vector Machine classification:\n"
                "1. Maps text to high-dimensional space using kernel function\n"
                "2. Finds optimal hyperplane separating sentiment classes\n"
                "3. Calculates probability based on distance from hyperplane"
            )

        ttk.Label(formula_frame, text=detail_text, justify=tk.LEFT, wraplength=500).pack(padx=10, pady=5)

    # Add update button at the top of the Numbers tab
    update_button = ttk.Button(tab, text="Update Visualization", command=update_viz)
    update_button.pack(pady=5)

    # Bind tab change event
    numbers_notebook.bind('<<NotebookTabChanged>>', lambda e: update_viz())

# Add the Numbers tab to the main notebook
create_numbers_tab(numbers_tab)

# Start the main event loop
root.mainloop()