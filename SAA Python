import re
from collections import defaultdict
from typing import List, Dict, Tuple
import tkinter as tk
from tkinter import messagebox, filedialog
from tkinter import ttk
import speech_recognition as sr
from pathlib import Path
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import numpy as np
from matplotlib.figure import Figure
import math
import csv

class SentimentAnalyzer:
    def __init__(self):
        self.sentiment_data = {
            'positive': [],
            'neutral': [],
            'negative': []
        }
        self.ngram_counts = {
            'bigram': defaultdict(lambda: defaultdict(int)),
            'trigram': defaultdict(lambda: defaultdict(int))
        }

        # Initialize Naive Bayes and SVM models
        self.vectorizer = TfidfVectorizer()
        self.nb_model = MultinomialNB()
        self.svm_model = make_pipeline(TfidfVectorizer(), SVC(probability=True))

        self.load_training_data()
        self.train()

    def load_training_data(self):
        # Load training data in CSV format
        training_data = [
            ["Sentence", "Sentiment"],  # Header row
            ["I absolutely love this product!", "positive"],
            ["This is the best day ever!", "positive"],
            ["Great service and amazing quality", "positive"],
            ["The weather is okay today", "neutral"],
            ["I'm not sure how I feel about this", "neutral"],
            ["It could be better but it's not terrible", "neutral"],
            ["This is terrible, I hate it", "negative"],
            ["Worst experience ever, very disappointing", "negative"],
            ["Poor quality and bad service", "negative"],
            ["I love my phone", "positive"],
            ["I hate my phone", "negative"],
            ["My phone is okay", "neutral"],
            ["I am very happy with my purchase", "positive"],
            ["I am very unhappy with my purchase", "negative"],
            ["The product is average", "neutral"]
        ]

        # Skip header row and process data
        for text, sentiment in training_data[1:]:
            self.sentiment_data[sentiment.lower()].append(text.lower())

    def preprocess_text(self, text: str) -> str:
        # Clean and preprocess text
        text = text.lower()
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        return text

    def get_ngrams(self, text: str, n: int) -> List[str]:
        # Generate n-grams from text
        words = self.preprocess_text(text).split()
        ngrams = []
        for i in range(len(words) - n + 1):
            ngram = ' '.join(words[i:i + n])
            ngrams.append(ngram)
        return ngrams

    def train(self):
        # Train the N-gram model
        for sentiment, texts in self.sentiment_data.items():
            for text in texts:
                # Process bigrams
                bigrams = self.get_ngrams(text, 2)
                for bigram in bigrams:
                    self.ngram_counts['bigram'][sentiment][bigram] += 1

                # Process trigrams
                trigrams = self.get_ngrams(text, 3)
                for trigram in trigrams:
                    self.ngram_counts['trigram'][sentiment][trigram] += 1

        # Train Naive Bayes and SVM models
        all_texts = []
        all_labels = []
        for sentiment, texts in self.sentiment_data.items():
            all_texts.extend(texts)
            all_labels.extend([sentiment] * len(texts))
        self.nb_model.fit(self.vectorizer.fit_transform(all_texts), all_labels)
        self.svm_model.fit(all_texts, all_labels)

    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        # Analyze sentiment using N-gram method
        bigrams = self.get_ngrams(text, 2)
        trigrams = self.get_ngrams(text, 3)
        
        scores_ngram = {
            'positive': 0.0,
            'neutral': 0.0,
            'negative': 0.0
        }

        bigram_weight = 0.75
        trigram_weight = 0.25

        for bigram in bigrams:
            for sentiment in scores_ngram.keys():
                if bigram in self.ngram_counts['bigram'][sentiment]:
                    scores_ngram[sentiment] += self.ngram_counts['bigram'][sentiment][bigram] * bigram_weight

        for trigram in trigrams:
            for sentiment in scores_ngram.keys():
                if trigram in self.ngram_counts['trigram'][sentiment]:
                    scores_ngram[sentiment] += self.ngram_counts['trigram'][sentiment][trigram] * trigram_weight

        if sum(scores_ngram.values()) == 0:
            scores_ngram['neutral'] = 1.0

        total_score_ngram = sum(scores_ngram.values()) or 1
        normalized_scores_ngram = {
            sentiment: (score / total_score_ngram) * 100 
            for sentiment, score in scores_ngram.items()
        }

        # Analyze sentiment using Naive Bayes method
        nb_probs = self.nb_model.predict_proba(self.vectorizer.transform([text]))[0]
        scores_nb = {
            'positive': nb_probs[self.nb_model.classes_.tolist().index('positive')] * 100,
            'neutral': nb_probs[self.nb_model.classes_.tolist().index('neutral')] * 100,
            'negative': nb_probs[self.nb_model.classes_.tolist().index('negative')] * 100
        }

        # Analyze sentiment using SVM method
        svm_probs = self.svm_model.predict_proba([text])[0]
        scores_svm = {
            'positive': svm_probs[self.svm_model.classes_.tolist().index('positive')] * 100,
            'neutral': svm_probs[self.svm_model.classes_.tolist().index('neutral')] * 100,
            'negative': svm_probs[self.svm_model.classes_.tolist().index('negative')] * 100
        }

        # Average the results for final consensus
        final_scores = {
            sentiment: (normalized_scores_ngram[sentiment] + scores_nb[sentiment] + scores_svm[sentiment]) / 3
            for sentiment in scores_ngram.keys()
        }

        return {
            'ngram': normalized_scores_ngram,
            'naive_bayes': scores_nb,
            'svm': scores_svm,
            'final': final_scores
        }

    def get_final_sentiment(self, scores: Dict[str, float]) -> Tuple[str, float]:
        final_sentiment = max(scores.items(), key=lambda x: x[1])
        return final_sentiment

    def add_to_training_data(self, text: str, sentiment: str):
        # Add new text to training data and retrain the model
        self.sentiment_data[sentiment].append(text.lower())
        self.train()

    def export_training_data(self, filepath: str):
        # Export training data to a CSV file
        with open(filepath, 'w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(["Sentence", "Sentiment"])  # Write header
            for sentiment, texts in self.sentiment_data.items():
                for text in texts:
                    writer.writerow([text, sentiment])

    def import_training_data(self, filepath: str):
        # Import training data from a CSV file
        if not Path(filepath).exists():
            messagebox.showerror("Error", f"File not found: {filepath}")
            return
        
        # Clear existing training data
        self.sentiment_data = {
            'positive': [],
            'neutral': [],
            'negative': []
        }
        
        try:
            with open(filepath, 'r', newline='') as file:
                reader = csv.DictReader(file)
                for row in reader:
                    sentiment = row['Sentiment'].lower()
                    text = row['Sentence'].lower()
                    if sentiment in self.sentiment_data:
                        self.sentiment_data[sentiment].append(text)
            self.train()
        except Exception as e:
            messagebox.showerror("Error", f"Error reading CSV file: {str(e)}")
            return

    def get_training_data(self) -> List[Tuple[str, str]]:
        # Get the training data as a list of tuples
        training_data = []
        for sentiment, texts in self.sentiment_data.items():
            for text in texts:
                training_data.append((text, sentiment))
        return training_data

def analyze_text():
    global last_analyzed_text
    text = text_entry.get("1.0", tk.END).strip()
    if not text:
        messagebox.showwarning("Input Error", "Please enter some text to analyze.")
        return

    # Store text before clearing
    current_text = text

    # Analyze sentiment
    scores = analyzer.analyze_sentiment(text)
    final_sentiment, confidence = analyzer.get_final_sentiment(scores['final'])

    # Display results in respective frames
    ngram_result_text = "N-gram Analysis:\n"
    for sentiment, score in scores['ngram'].items():
        ngram_result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
    ngram_result_label.config(text=ngram_result_text)

    nb_result_text = "Naive Bayes Analysis:\n"
    for sentiment, score in scores['naive_bayes'].items():
        nb_result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
    nb_result_label.config(text=nb_result_text)

    svm_result_text = "SVM Analysis:\n"
    for sentiment, score in scores['svm'].items():
        svm_result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
    svm_result_label.config(text=svm_result_text)

    final_result_text = f"Final Consensus:\n"
    for sentiment, score in scores['final'].items():
        final_result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
    final_result_text += f"\nFinal Sentiment: {final_sentiment.capitalize()} (Confidence: {confidence:.2f}%)"
    final_result_label.config(text=final_result_text)

    # Store the last analyzed text
    last_analyzed_text = text

    # Update Numbers tab visualizations with the current text and scores
    for child in tab_control.winfo_children():
        if tab_control.tab(child)['text'] == 'Numbers':
            update_numbers_tab(child, current_text, scores)

    # Clear the text box
    text_entry.delete("1.0", tk.END)

    # Update visualizations in Numbers tab
    for child in tab_control.winfo_children():
        if tab_control.tab(child)['text'] == 'Numbers':
            for notebook in child.winfo_children():
                if isinstance(notebook, ttk.Notebook):
                    for tab in notebook.winfo_children():
                        for button in tab.winfo_children():
                            if isinstance(button, ttk.Button):
                                button.invoke()

def update_numbers_tab(tab, text, scores):
    # Find the notebook within the Numbers tab
    numbers_notebook = None
    for child in tab.winfo_children():
        if isinstance(child, ttk.Notebook):
            numbers_notebook = child
            break
    
    if not numbers_notebook:
        return

    # Update each visualization based on the active tab
    current_tab = numbers_notebook.select()
    if current_tab:
        tab_name = numbers_notebook.tab(current_tab)['text']
        
        if tab_name == "N-gram Details":
            update_ngram_details(current_tab, text, scores)
        elif tab_name == "Naive Bayes Details":
            update_nb_details(current_tab, text, scores)
        elif tab_name == "SVM Details":
            update_svm_details(current_tab, text, scores)

def update_ngram_details(tab, text, scores):
    # Clear existing content
    for widget in tab.winfo_children():
        widget.destroy()

    # Create figure
    fig = Figure(figsize=(10, 6))
    ax = fig.add_subplot(111)

    # Plot N-gram scores
    sentiments = list(scores['ngram'].keys())
    values = [scores['ngram'][s] for s in sentiments]
    x = np.arange(len(sentiments))
    
    bars = ax.bar(x, values)
    ax.set_xticks(x)
    ax.set_xticklabels(sentiments)
    ax.set_title('N-gram Analysis Scores')
    ax.set_ylabel('Probability (%)')

    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%', ha='center', va='bottom')

    canvas = FigureCanvasTkAgg(fig, master=tab)
    canvas.draw()
    canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)

    # Add detailed breakdown
    details_frame = ttk.LabelFrame(tab, text="Detailed Breakdown")
    details_frame.pack(fill=tk.X, padx=5, pady=5)
    
    bigrams = analyzer.get_ngrams(text, 2)
    trigrams = analyzer.get_ngrams(text, 3)
    
    ttk.Label(details_frame, text="Bigrams found:").pack(anchor=tk.W)
    for bigram in bigrams:
        ttk.Label(details_frame, text=f"• {bigram}").pack(anchor=tk.W)
    
    ttk.Label(details_frame, text="\nTrigrams found:").pack(anchor=tk.W)
    for trigram in trigrams:
        ttk.Label(details_frame, text=f"• {trigram}").pack(anchor=tk.W)

def update_nb_details(tab, text, scores):
    # Clear existing content
    for widget in tab.winfo_children():
        widget.destroy()

    # Create figure
    fig = Figure(figsize=(10, 6))
    ax = fig.add_subplot(111)

    # Plot Naive Bayes scores
    sentiments = list(scores['naive_bayes'].keys())
    values = [scores['naive_bayes'][s] for s in sentiments]
    x = np.arange(len(sentiments))
    
    bars = ax.bar(x, values)
    ax.set_xticks(x)
    ax.set_xticklabels(sentiments)
    ax.set_title('Naive Bayes Probability Distribution')
    ax.set_ylabel('Probability (%)')

    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%', ha='center', va='bottom')

    canvas = FigureCanvasTkAgg(fig, master=tab)
    canvas.draw()
    canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)

    # Add word probability breakdown
    details_frame = ttk.LabelFrame(tab, text="Word Probabilities")
    details_frame.pack(fill=tk.X, padx=5, pady=5)
    
    # Get word probabilities from the vectorizer
    words = analyzer.vectorizer.get_feature_names_out()
    for word in words:
        if word in text.lower():
            ttk.Label(details_frame, 
                     text=f"'{word}' probabilities:").pack(anchor=tk.W)
            for sentiment in sentiments:
                prob = analyzer.nb_model.feature_log_prob_[
                    list(analyzer.nb_model.classes_).index(sentiment)
                ][list(words).index(word)]
                ttk.Label(details_frame, 
                         text=f"  {sentiment}: {np.exp(prob):.4f}").pack(anchor=tk.W)

def update_svm_details(tab, text, scores):
    # Clear existing content
    for widget in tab.winfo_children():
        widget.destroy()

    # Create figure
    fig = Figure(figsize=(10, 6))
    ax = fig.add_subplot(111)

    # Plot SVM scores
    sentiments = list(scores['svm'].keys())
    values = [scores['svm'][s] for s in sentiments]
    x = np.arange(len(sentiments))
    
    bars = ax.bar(x, values)
    ax.set_xticks(x)
    ax.set_xticklabels(sentiments)
    ax.set_title('SVM Classification Probabilities')
    ax.set_ylabel('Probability (%)')

    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%', ha='center', va='bottom')

    canvas = FigureCanvasTkAgg(fig, master=tab)
    canvas.draw()
    canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)

    # Add SVM details
    details_frame = ttk.LabelFrame(tab, text="SVM Analysis Details")
    details_frame.pack(fill=tk.X, padx=5, pady=5)
    
    # Get decision function values
    X = analyzer.svm_model.named_steps['tfidfvectorizer'].transform([text])
    decision_values = analyzer.svm_model.named_steps['svc'].decision_function(X)
    
    ttk.Label(details_frame, text="Decision Function Values:").pack(anchor=tk.W)
    for i, sentiment in enumerate(sentiments):
        ttk.Label(details_frame, 
                 text=f"• {sentiment}: {decision_values[0][i]:.4f}").pack(anchor=tk.W)

def add_to_training():
    global last_analyzed_text
    if not last_analyzed_text:
        messagebox.showwarning("Input Error", "No text has been analyzed yet.")
        return

    sentiment = sentiment_var.get()
    if sentiment not in ['positive', 'neutral', 'negative']:
        messagebox.showwarning("Input Error", "Please select a sentiment.")
        return

    analyzer.add_to_training_data(last_analyzed_text, sentiment)
    messagebox.showinfo("Success", "Text added to training data successfully.")
    update_training_data_view()

def export_training_data():
    filepath = filedialog.asksaveasfilename(
        defaultextension=".csv",
        filetypes=[("CSV files", "*.csv")]
    )
    if filepath:
        analyzer.export_training_data(filepath)
        messagebox.showinfo("Success", "Training data exported successfully.")

def import_training_data():
    filepath = filedialog.askopenfilename(
        filetypes=[("CSV files", "*.csv")]
    )
    if filepath:
        analyzer.import_training_data(filepath)
        messagebox.showinfo("Success", "Training data imported successfully.")
        update_training_data_view()

def update_training_data_view():
    training_data = analyzer.get_training_data()
    training_data_text = "\n".join([f"{text} - {sentiment}" for text, sentiment in training_data])
    training_data_textbox.config(state=tk.NORMAL)
    training_data_textbox.delete("1.0", tk.END)
    training_data_textbox.insert(tk.END, training_data_text)
    training_data_textbox.config(state=tk.DISABLED)

# Initialize analyzer
analyzer = SentimentAnalyzer()
last_analyzed_text = ""

# Create the main window
root = tk.Tk()
root.title("Sentiment Analyzer")

# Create a tabbed interface
tab_control = ttk.Notebook(root)
tab1 = ttk.Frame(tab_control)
tab2 = ttk.Frame(tab_control)
tab3 = ttk.Frame(tab_control)
tab_control.add(tab3, text="Audio Analysis")      # Changed order - Audio first
tab_control.add(tab1, text="Text Analysis")  # Text analysis second
tab_control.add(tab2, text="Training Data")       # Training data last
tab_control.pack(expand=1, fill="both")

# Tab 1: Sentiment Analysis
tk.Label(tab1, text="Enter text to analyze:").pack(pady=10)
text_entry = tk.Text(tab1, height=10, width=50)
text_entry.pack(pady=10)
analyze_button = tk.Button(tab1, text="Analyze Sentiment", command=analyze_text)
analyze_button.pack(pady=10)

# Create a frame for the results
results_frame = tk.Frame(tab1)
results_frame.pack(pady=10)

# N-gram Analysis
ngram_frame = tk.Frame(results_frame, borderwidth=1, relief="solid")
ngram_frame.pack(side=tk.LEFT, padx=5, pady=5)
tk.Label(ngram_frame, text="N-gram Analysis").pack()
ngram_result_label = tk.Label(ngram_frame, text="", justify=tk.LEFT)
ngram_result_label.pack()

# Naive Bayes Analysis
nb_frame = tk.Frame(results_frame, borderwidth=1, relief="solid")
nb_frame.pack(side=tk.LEFT, padx=5, pady=5)
tk.Label(nb_frame, text="Naive Bayes Analysis").pack()
nb_result_label = tk.Label(nb_frame, text="", justify=tk.LEFT)
nb_result_label.pack()

# SVM Analysis
svm_frame = tk.Frame(results_frame, borderwidth=1, relief="solid")
svm_frame.pack(side=tk.LEFT, padx=5, pady=5)
tk.Label(svm_frame, text="SVM Analysis").pack()
svm_result_label = tk.Label(svm_frame, text="", justify=tk.LEFT)
svm_result_label.pack()

# Final Consensus
final_frame = tk.Frame(tab1, borderwidth=1, relief="solid")
final_frame.pack(pady=10)
tk.Label(final_frame, text="Final Consensus").pack()
final_result_label = tk.Label(final_frame, text="", justify=tk.LEFT)
final_result_label.pack()

# Define result_label to avoid the error
result_label = tk.Label(tab1, text="", justify=tk.LEFT)
result_label.pack(pady=10)

# Create a frame for the buttons and sentiment selection
top_frame = tk.Frame(tab1)
top_frame.pack(pady=10)

# Add text above the sentiment selection
tk.Label(top_frame, text="Add results to training data as:").pack(side=tk.TOP)

# Add sentiment selection for training data
sentiment_var = tk.StringVar(value="neutral")
tk.Radiobutton(top_frame, text="Positive", variable=sentiment_var, value="positive").pack(side=tk.TOP)
tk.Radiobutton(top_frame, text="Neutral", variable=sentiment_var, value="neutral").pack(side=tk.TOP)
tk.Radiobutton(top_frame, text="Negative", variable=sentiment_var, value="negative").pack(side=tk.TOP)

add_button = tk.Button(top_frame, text="Add results", command=add_to_training)
add_button.pack(side=tk.TOP, pady=5)

# Create a frame for the import and export buttons at the bottom
bottom_frame = tk.Frame(tab1)
bottom_frame.pack(side=tk.BOTTOM, pady=10)

import_export_frame = tk.Frame(bottom_frame)
import_export_frame.pack(side=tk.TOP, pady=5)

export_button = tk.Button(import_export_frame, text="Export Training Data", command=export_training_data)
export_button.pack(side=tk.LEFT, padx=5)

import_button = tk.Button(import_export_frame, text="Import Training Data", command=import_training_data)
import_button.pack(side=tk.LEFT, padx=5)

# Tab 2: Training Data
training_data_textbox = tk.Text(tab2, height=20, width=80, state=tk.DISABLED)
training_data_textbox.pack(pady=10)

# Update the training data view initially
update_training_data_view()

def process_audio_file():
    file_path = filedialog.askopenfilename(
        filetypes=[("Audio Files", "*.wav *.mp3 *.ogg *.m4a")]  # Allow more audio formats
    )
    if not file_path:
        return
    
    # Check if file exists
    if not Path(file_path).exists():
        messagebox.showerror("Error", f"File not found: {file_path}")
        audio_status_label.config(text="Error: File not found")
        return
        
    # Debug info
    print(f"Selected file: {file_path}")
    print(f"File exists: {Path(file_path).exists()}")
    print(f"File size: {Path(file_path).stat().st_size} bytes")
    
    audio_status_label.config(text=f"Processing audio file: {Path(file_path).name}")
    root.update()
    
    try:
        # Initialize recognizer with specific settings
        r = sr.Recognizer()
        r.energy_threshold = 4000
        r.dynamic_energy_threshold = True
        
        # Load the audio file
        try:
            with sr.AudioFile(file_path) as source:
                print("Audio file opened successfully")
                # Adjust for ambient noise
                r.adjust_for_ambient_noise(source)
                print("Ambient noise adjustment complete")
                audio = r.record(source)
                print("Audio recording complete")
        except Exception as e:
            print(f"Error opening audio file: {str(e)}")
            raise
            
        try:
            # Convert speech to text
            text = r.recognize_google(audio)
            
            # Display the transcribed text
            audio_text_display.config(state=tk.NORMAL)
            audio_text_display.delete(1.0, tk.END)
            audio_text_display.insert(tk.END, text)
            audio_text_display.config(state=tk.DISABLED)
            
            # Only proceed with sentiment analysis if we have text
            if text.strip():
                # Analyze sentiment
                scores = analyzer.analyze_sentiment(text)
                final_sentiment, confidence = analyzer.get_final_sentiment(scores['final'])
                
                # Display results
                result_text = f"Sentiment Analysis Results:\n{'-' * 25}\n\nDetailed Scores:\n"
                result_text += "N-gram Analysis:\n"
                for sentiment, score in scores['ngram'].items():
                    result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
                result_text += "\nNaive Bayes Analysis:\n"
                for sentiment, score in scores['naive_bayes'].items():
                    result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
                result_text += "\nSVM Analysis:\n"
                for sentiment, score in scores['svm'].items():
                    result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
                result_text += f"\nFinal Consensus:\n"
                for sentiment, score in scores['final'].items():
                    result_text += f"{sentiment.capitalize()}: {score:.2f}%\n"
                result_text += f"\nFinal Sentiment: {final_sentiment.capitalize()} (Confidence: {confidence:.2f}%)"
                
                audio_result_label.config(text=result_text)
                audio_status_label.config(text="Analysis complete!")
            else:
                audio_status_label.config(text="No speech detected in audio")
                
        except sr.UnknownValueError:
            messagebox.showerror("Error", "Could not understand the audio. Please ensure the audio is clear and contains speech.")
            audio_status_label.config(text="Error: Could not understand the audio")
        except sr.RequestError as e:
            messagebox.showerror("Error", f"Could not connect to the speech recognition service: {str(e)}")
            audio_status_label.config(text="Error: Service connection failed")
            
    except Exception as e:
        messagebox.showerror("Error", f"An error occurred while processing the audio file: {str(e)}")
        audio_status_label.config(text="Error: Processing failed")
        print(f"Detailed error: {str(e)}")  # For debugging

# Tab 3: Audio Analysis
tk.Label(tab3, text="Select an audio file for analysis:").pack(pady=10)
audio_button = tk.Button(tab3, text="Choose Audio File", command=process_audio_file)
audio_button.pack(pady=10)

audio_status_label = tk.Label(tab3, text="No file selected")
audio_status_label.pack(pady=5)

tk.Label(tab3, text="Transcribed Text:").pack(pady=5)
audio_text_display = tk.Text(tab3, height=8, width=50, state=tk.DISABLED)
audio_text_display.pack(pady=10)

audio_result_label = tk.Label(tab3, text="", justify=tk.LEFT)
audio_result_label.pack(pady=10)

# Add training controls to Audio tab
audio_top_frame = tk.Frame(tab3)
audio_top_frame.pack(pady=10)

# Add text above the sentiment selection
tk.Label(audio_top_frame, text="Add results to training data as:").pack(side=tk.TOP)

# Add sentiment selection for training data
audio_sentiment_var = tk.StringVar(value="neutral")
tk.Radiobutton(audio_top_frame, text="Positive", variable=audio_sentiment_var, value="positive").pack(side=tk.TOP)
tk.Radiobutton(audio_top_frame, text="Neutral", variable=audio_sentiment_var, value="neutral").pack(side=tk.TOP)
tk.Radiobutton(audio_top_frame, text="Negative", variable=audio_sentiment_var, value="negative").pack(side=tk.TOP)

audio_add_button = tk.Button(audio_top_frame, text="Add results", command=lambda: add_to_training_from_audio())
audio_add_button.pack(side=tk.TOP, pady=5)

# Create a frame for the import and export buttons at the bottom
audio_bottom_frame = tk.Frame(tab3)
audio_bottom_frame.pack(side=tk.BOTTOM, pady=10)

audio_import_export_frame = tk.Frame(audio_bottom_frame)
audio_import_export_frame.pack(side=tk.TOP, pady=5)

audio_export_button = tk.Button(audio_import_export_frame, text="Export Training Data", command=export_training_data)
audio_export_button.pack(side=tk.LEFT, padx=5)

audio_import_button = tk.Button(audio_import_export_frame, text="Import Training Data", command=import_training_data)
audio_import_button.pack(side=tk.LEFT, padx=5)

# Add new function to handle adding audio transcription to training data
def add_to_training_from_audio():
    text = audio_text_display.get("1.0", tk.END).strip()
    if not text:
        messagebox.showwarning("Input Error", "No text has been transcribed yet.")
        return

    sentiment = audio_sentiment_var.get()
    if sentiment not in ['positive', 'neutral', 'negative']:
        messagebox.showwarning("Input Error", "Please select a sentiment.")
        return

    analyzer.add_to_training_data(text, sentiment)
    messagebox.showinfo("Success", "Text added to training data successfully.")
    update_training_data_view()

def create_about_tab(tab):
    # Create a scrollable frame
    canvas = tk.Canvas(tab)
    scrollbar = ttk.Scrollbar(tab, orient="vertical", command=canvas.yview)
    scrollable_frame = ttk.Frame(canvas)

    scrollable_frame.bind(
        "<Configure>",
        lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
    )

    canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
    canvas.configure(yscrollcommand=scrollbar.set)

    # Title
    title = ttk.Label(
        scrollable_frame,
        text="About the Sentiment Analyzer",
        font=("Helvetica", 14, "bold")
    )
    title.pack(pady=10)

    # What is Sentiment Analysis section
    what_is = ttk.LabelFrame(scrollable_frame, text="What is Sentiment Analysis?")
    ttk.Label(
        what_is,
        text="Sentiment analysis is the process of determining the emotional tone behind "
             "a piece of text. This tool provides two different approaches to sentiment analysis:",
        wraplength=500
    ).pack(pady=5)
    
    approaches = ttk.Frame(what_is)
    ttk.Label(approaches, text="• Naive Bayes Classification (word-based)").pack(anchor="w")
    ttk.Label(approaches, text="• N-gram Analysis (phrase-based)").pack(anchor="w")
    approaches.pack(pady=5)
    what_is.pack(fill="x", padx=10, pady=5)

    # Naive Bayes Analysis section
    naive_bayes = ttk.LabelFrame(scrollable_frame, text="Naive Bayes Analysis")
    ttk.Label(
        naive_bayes,
        text="The Naive Bayes classifier uses Bayes' Theorem with a \"naive\" assumption "
             "of independence between words. The mathematical formula is:",
        wraplength=500
    ).pack(pady=5)
    
    formula = ttk.Label(
        naive_bayes,
        text="P(class|text) ∝ P(class) × ∏ P(word|class)",
        font=("Courier", 10)
    )
    formula.pack(pady=5)
    
    components = ttk.Frame(naive_bayes)
    ttk.Label(components, text="• P(class) - Prior probability of each class (positive/negative/neutral)").pack(anchor="w")
    ttk.Label(components, text="• P(word|class) - Probability of each word appearing in a given class").pack(anchor="w")
    ttk.Label(components, text="• ∏ - Product of all word probabilities").pack(anchor="w")
    components.pack(pady=5)
    naive_bayes.pack(fill="x", padx=10, pady=5)

    # N-gram Analysis section
    ngram = ttk.LabelFrame(scrollable_frame, text="N-gram Analysis")
    ttk.Label(
        ngram,
        text="N-gram analysis extends beyond individual words to consider sequences of N consecutive words:",
        wraplength=500
    ).pack(pady=5)
    
    ngram_types = ttk.Frame(ngram)
    ttk.Label(ngram_types, text="• Unigrams (N=1): Individual words").pack(anchor="w")
    ttk.Label(ngram_types, text="• Bigrams (N=2): Pairs of consecutive words").pack(anchor="w")
    ttk.Label(ngram_types, text="• Trigrams (N=3): Sequences of three consecutive words").pack(anchor="w")
    ngram_types.pack(pady=5)
    ngram.pack(fill="x", padx=10, pady=5)

    # Key Features section
    features = ttk.LabelFrame(scrollable_frame, text="Key Features")
    features_list = [
        "• Dual analysis methods (Naive Bayes and N-grams)",
        "• Laplace (add-one) smoothing for both approaches",
        "• Log probabilities to prevent numerical underflow",
        "• Confidence scores using probability normalization",
        "• Interactive training data management",
        "• CSV import/export capabilities",
        "• Detailed probability calculations view"
    ]
    for feature in features_list:
        ttk.Label(features, text=feature).pack(anchor="w")
    features.pack(fill="x", padx=10, pady=5)

    # Text Processing section
    processing = ttk.LabelFrame(scrollable_frame, text="Text Processing")
    process_steps = [
        "1. Text is converted to lowercase",
        "2. Punctuation is removed",
        "3. Text is split into tokens (words or n-grams)",
        "4. Empty tokens are filtered out"
    ]
    for step in process_steps:
        ttk.Label(processing, text=step).pack(anchor="w")
    processing.pack(fill="x", padx=10, pady=5)

    # Training Process section
    training = ttk.LabelFrame(scrollable_frame, text="Training Process")
    train_steps = [
        "1. Count occurrences of tokens in each class",
        "2. Calculate prior probabilities for each class",
        "3. Apply Laplace smoothing to token probabilities",
        "4. Store model parameters for both analysis methods"
    ]
    for step in train_steps:
        ttk.Label(training, text=step).pack(anchor="w")
    training.pack(fill="x", padx=10, pady=5)

    # Prediction Process section
    prediction = ttk.LabelFrame(scrollable_frame, text="Prediction Process")
    pred_steps = [
        "1. Preprocess input text",
        "2. Generate appropriate tokens (words or n-grams)",
        "3. Calculate log probabilities for each class",
        "4. Sum log probabilities of tokens and prior",
        "5. Convert to normalized confidence scores",
        "6. Select class with highest probability"
    ]
    for step in pred_steps:
        ttk.Label(prediction, text=step).pack(anchor="w")
    prediction.pack(fill="x", padx=10, pady=5)

    # Pack the canvas and scrollbar
    canvas.pack(side="left", fill="both", expand=True, padx=5, pady=5)
    scrollbar.pack(side="right", fill="y")

# Modify your existing code to add the About tab
tab4 = ttk.Frame(tab_control)
tab_control.add(tab4, text="About")
create_about_tab(tab4)

def create_numbers_tab(tab):
    # Create a notebook for sub-tabs within Numbers tab
    numbers_notebook = ttk.Notebook(tab)
    
    # Create sub-tabs for each model
    ngram_tab = ttk.Frame(numbers_notebook)
    nb_tab = ttk.Frame(numbers_notebook)
    svm_tab = ttk.Frame(numbers_notebook)
    
    numbers_notebook.add(ngram_tab, text="N-gram Details")
    numbers_notebook.add(nb_tab, text="Naive Bayes Details")
    numbers_notebook.add(svm_tab, text="SVM Details")
    numbers_notebook.pack(expand=1, fill="both")

    # N-gram Analysis Tab
    def update_ngram_viz():
        text = text_entry.get("1.0", tk.END).strip()
        if not text:
            return

        # Clear previous plots
        for widget in ngram_frame.winfo_children():
            widget.destroy()

        # Create figure for N-gram visualization
        fig = Figure(figsize=(10, 6))
        ax = fig.add_subplot(111)

        # Get N-gram counts
        bigrams = analyzer.get_ngrams(text, 2)
        trigrams = analyzer.get_ngrams(text, 3)

        # Calculate probabilities
        sentiments = ['positive', 'neutral', 'negative']
        bigram_probs = {s: [] for s in sentiments}
        trigram_probs = {s: [] for s in sentiments}

        for b in bigrams:
            for s in sentiments:
                prob = analyzer.ngram_counts['bigram'][s].get(b, 0)
                bigram_probs[s].append(prob)

        for t in trigrams:
            for s in sentiments:
                prob = analyzer.ngram_counts['trigram'][s].get(t, 0)
                trigram_probs[s].append(prob)

        # Plot
        x = np.arange(len(bigrams))
        width = 0.25

        for i, s in enumerate(sentiments):
            ax.bar(x + i*width, bigram_probs[s], width, label=f'{s} (bigrams)')

        ax.set_xticks(x + width)
        ax.set_xticklabels(bigrams, rotation=45, ha='right')
        ax.legend()
        ax.set_title('N-gram Probability Distribution')

        canvas = FigureCanvasTkAgg(fig, master=ngram_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)

        # Add formula explanation
        formula_text = (
            "N-gram Model Formula:\n"
            "P(sentiment|text) = Σ(w_b * P(bigram|sentiment) + w_t * P(trigram|sentiment))\n"
            "where w_b=0.75 and w_t=0.25 are bigram and trigram weights"
        )
        tk.Label(ngram_frame, text=formula_text, justify=tk.LEFT).pack(pady=10)

    # Naive Bayes Tab
    def update_nb_viz():
        text = text_entry.get("1.0", tk.END).strip()
        if not text:
            return

        # Clear previous plots
        for widget in nb_frame.winfo_children():
            widget.destroy()

        # Create figure for NB visualization
        fig = Figure(figsize=(10, 6))
        ax = fig.add_subplot(111)

        # Get word probabilities
        X = analyzer.vectorizer.transform([text])
        feature_names = analyzer.vectorizer.get_feature_names_out()
        word_probs = analyzer.nb_model.feature_log_prob_

        # Plot word probabilities
        x = np.arange(len(feature_names))
        width = 0.25

        for i, sentiment in enumerate(['positive', 'neutral', 'negative']):
            ax.bar(x + i*width, np.exp(word_probs[i]), width, label=sentiment)

        ax.set_xticks(x + width)
        ax.set_xticklabels(feature_names, rotation=45, ha='right')
        ax.legend()
        ax.set_title('Word Probability Distribution (Naive Bayes)')

        canvas = FigureCanvasTkAgg(fig, master=nb_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)

        # Add formula explanation
        formula_text = (
            "Naive Bayes Formula:\n"
            "P(sentiment|text) = P(sentiment) * Π P(word|sentiment)\n"
            "Log space: log(P(sentiment|text)) = log(P(sentiment)) + Σ log(P(word|sentiment))"
        )
        tk.Label(nb_frame, text=formula_text, justify=tk.LEFT).pack(pady=10)

    # SVM Tab
    def update_svm_viz():
        text = text_entry.get("1.0", tk.END).strip()
        if not text:
            return

        # Clear previous plots
        for widget in svm_frame.winfo_children():
            widget.destroy()

        # Create figure for SVM visualization
        fig = Figure(figsize=(10, 6))
        ax = fig.add_subplot(111)

        # Get SVM decision function values
        X = analyzer.svm_model.named_steps['tfidfvectorizer'].transform([text])
        decision_values = analyzer.svm_model.named_steps['svc'].decision_function(X)

        # Plot decision boundaries
        sentiments = ['positive', 'neutral', 'negative']
        x = np.arange(len(sentiments))
        
        ax.bar(x, decision_values[0], width=0.5)
        ax.set_xticks(x)
        ax.set_xticklabels(sentiments)
        ax.set_title('SVM Decision Function Values')

        canvas = FigureCanvasTkAgg(fig, master=svm_frame)
        canvas.draw()
        canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)

        # Add formula explanation
        formula_text = (
            "SVM Formula:\n"
            "f(x) = Σ(α_i * K(x_i, x) + b)\n"
            "where K is the kernel function and α_i are the support vector coefficients"
        )
        tk.Label(svm_frame, text=formula_text, justify=tk.LEFT).pack(pady=10)

    # Create frames for each analysis type
    ngram_frame = ttk.Frame(ngram_tab)
    nb_frame = ttk.Frame(nb_tab)
    svm_frame = ttk.Frame(svm_tab)

    ngram_frame.pack(fill=tk.BOTH, expand=1)
    nb_frame.pack(fill=tk.BOTH, expand=1)
    svm_frame.pack(fill=tk.BOTH, expand=1)

    # Add update buttons
    ttk.Button(ngram_frame, text="Update N-gram Analysis", command=update_ngram_viz).pack(pady=5)
    ttk.Button(nb_frame, text="Update Naive Bayes Analysis", command=update_nb_viz).pack(pady=5)
    ttk.Button(svm_frame, text="Update SVM Analysis", command=update_svm_viz).pack(pady=5)

# Add the Numbers tab to the main notebook
tab5 = ttk.Frame(tab_control)
tab_control.add(tab5, text="Numbers")
create_numbers_tab(tab5)

# Start the main event loop
root.mainloop()